[{"path":"https://evolecolgroup.github.io/tidysdm/dev/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"GNU Affero General Public License","title":"GNU Affero General Public License","text":"Version 3, 19 November 2007 Copyright (C) 2007 Free Software Foundation, Inc. <https://fsf.org/> Everyone permitted copy distribute verbatim copies license document, changing allowed.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/LICENSE.html","id":"preamble","dir":"","previous_headings":"","what":"Preamble","title":"GNU Affero General Public License","text":"GNU Affero General Public License free, copyleft license software kinds works, specifically designed ensure cooperation community case network server software. licenses software practical works designed take away freedom share change works. contrast, General Public Licenses intended guarantee freedom share change versions program–make sure remains free software users. speak free software, referring freedom, price. General Public Licenses designed make sure freedom distribute copies free software (charge wish), receive source code can get want , can change software use pieces new free programs, know can things. Developers use General Public Licenses protect rights two steps: (1) assert copyright software, (2) offer License gives legal permission copy, distribute /modify software. secondary benefit defending users’ freedom improvements made alternate versions program, receive widespread use, become available developers incorporate. Many developers free software heartened encouraged resulting cooperation. However, case software used network servers, result may fail come . GNU General Public License permits making modified version letting public access server without ever releasing source code public. GNU Affero General Public License designed specifically ensure , cases, modified source code becomes available community. requires operator network server provide source code modified version running users server. Therefore, public use modified version, publicly accessible server, gives public access source code modified version. older license, called Affero General Public License published Affero, designed accomplish similar goals. different license, version Affero GPL, Affero released new version Affero GPL permits relicensing license. precise terms conditions copying, distribution modification follow.","code":""},{"path":[]},{"path":"https://evolecolgroup.github.io/tidysdm/dev/LICENSE.html","id":"id_0-definitions","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"0. Definitions.","title":"GNU Affero General Public License","text":"“License” refers version 3 GNU Affero General Public License. “Copyright” also means copyright-like laws apply kinds works, semiconductor masks. “Program” refers copyrightable work licensed License. licensee addressed “”. “Licensees” “recipients” may individuals organizations. “modify” work means copy adapt part work fashion requiring copyright permission, making exact copy. resulting work called “modified version” earlier work work “based ” earlier work. “covered work” means either unmodified Program work based Program. “propagate” work means anything , without permission, make directly secondarily liable infringement applicable copyright law, except executing computer modifying private copy. Propagation includes copying, distribution (without modification), making available public, countries activities well. “convey” work means kind propagation enables parties make receive copies. Mere interaction user computer network, transfer copy, conveying. interactive user interface displays “Appropriate Legal Notices” extent includes convenient prominently visible feature (1) displays appropriate copyright notice, (2) tells user warranty work (except extent warranties provided), licensees may convey work License, view copy License. interface presents list user commands options, menu, prominent item list meets criterion.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/LICENSE.html","id":"id_1-source-code","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"1. Source Code.","title":"GNU Affero General Public License","text":"“source code” work means preferred form work making modifications . “Object code” means non-source form work. “Standard Interface” means interface either official standard defined recognized standards body, , case interfaces specified particular programming language, one widely used among developers working language. “System Libraries” executable work include anything, work whole, () included normal form packaging Major Component, part Major Component, (b) serves enable use work Major Component, implement Standard Interface implementation available public source code form. “Major Component”, context, means major essential component (kernel, window system, ) specific operating system () executable work runs, compiler used produce work, object code interpreter used run . “Corresponding Source” work object code form means source code needed generate, install, (executable work) run object code modify work, including scripts control activities. However, include work’s System Libraries, general-purpose tools generally available free programs used unmodified performing activities part work. example, Corresponding Source includes interface definition files associated source files work, source code shared libraries dynamically linked subprograms work specifically designed require, intimate data communication control flow subprograms parts work. Corresponding Source need include anything users can regenerate automatically parts Corresponding Source. Corresponding Source work source code form work.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/LICENSE.html","id":"id_2-basic-permissions","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"2. Basic Permissions.","title":"GNU Affero General Public License","text":"rights granted License granted term copyright Program, irrevocable provided stated conditions met. License explicitly affirms unlimited permission run unmodified Program. output running covered work covered License output, given content, constitutes covered work. License acknowledges rights fair use equivalent, provided copyright law. may make, run propagate covered works convey, without conditions long license otherwise remains force. may convey covered works others sole purpose make modifications exclusively , provide facilities running works, provided comply terms License conveying material control copyright. thus making running covered works must exclusively behalf, direction control, terms prohibit making copies copyrighted material outside relationship . Conveying circumstances permitted solely conditions stated . Sublicensing allowed; section 10 makes unnecessary.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/LICENSE.html","id":"id_3-protecting-users-legal-rights-from-anti-circumvention-law","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"3. Protecting Users’ Legal Rights From Anti-Circumvention Law.","title":"GNU Affero General Public License","text":"covered work shall deemed part effective technological measure applicable law fulfilling obligations article 11 WIPO copyright treaty adopted 20 December 1996, similar laws prohibiting restricting circumvention measures. convey covered work, waive legal power forbid circumvention technological measures extent circumvention effected exercising rights License respect covered work, disclaim intention limit operation modification work means enforcing, work’s users, third parties’ legal rights forbid circumvention technological measures.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/LICENSE.html","id":"id_4-conveying-verbatim-copies","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"4. Conveying Verbatim Copies.","title":"GNU Affero General Public License","text":"may convey verbatim copies Program’s source code receive , medium, provided conspicuously appropriately publish copy appropriate copyright notice; keep intact notices stating License non-permissive terms added accord section 7 apply code; keep intact notices absence warranty; give recipients copy License along Program. may charge price price copy convey, may offer support warranty protection fee.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/LICENSE.html","id":"id_5-conveying-modified-source-versions","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"5. Conveying Modified Source Versions.","title":"GNU Affero General Public License","text":"may convey work based Program, modifications produce Program, form source code terms section 4, provided also meet conditions: work must carry prominent notices stating modified , giving relevant date. work must carry prominent notices stating released License conditions added section 7. requirement modifies requirement section 4 “keep intact notices”. must license entire work, whole, License anyone comes possession copy. License therefore apply, along applicable section 7 additional terms, whole work, parts, regardless packaged. License gives permission license work way, invalidate permission separately received . work interactive user interfaces, must display Appropriate Legal Notices; however, Program interactive interfaces display Appropriate Legal Notices, work need make . compilation covered work separate independent works, nature extensions covered work, combined form larger program, volume storage distribution medium, called “aggregate” compilation resulting copyright used limit access legal rights compilation’s users beyond individual works permit. Inclusion covered work aggregate cause License apply parts aggregate.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/LICENSE.html","id":"id_6-conveying-non-source-forms","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"6. Conveying Non-Source Forms.","title":"GNU Affero General Public License","text":"may convey covered work object code form terms sections 4 5, provided also convey machine-readable Corresponding Source terms License, one ways: Convey object code , embodied , physical product (including physical distribution medium), accompanied Corresponding Source fixed durable physical medium customarily used software interchange. Convey object code , embodied , physical product (including physical distribution medium), accompanied written offer, valid least three years valid long offer spare parts customer support product model, give anyone possesses object code either (1) copy Corresponding Source software product covered License, durable physical medium customarily used software interchange, price reasonable cost physically performing conveying source, (2) access copy Corresponding Source network server charge. Convey individual copies object code copy written offer provide Corresponding Source. alternative allowed occasionally noncommercially, received object code offer, accord subsection 6b. Convey object code offering access designated place (gratis charge), offer equivalent access Corresponding Source way place charge. need require recipients copy Corresponding Source along object code. place copy object code network server, Corresponding Source may different server (operated third party) supports equivalent copying facilities, provided maintain clear directions next object code saying find Corresponding Source. Regardless server hosts Corresponding Source, remain obligated ensure available long needed satisfy requirements. Convey object code using peer--peer transmission, provided inform peers object code Corresponding Source work offered general public charge subsection 6d. separable portion object code, whose source code excluded Corresponding Source System Library, need included conveying object code work. “User Product” either (1) “consumer product”, means tangible personal property normally used personal, family, household purposes, (2) anything designed sold incorporation dwelling. determining whether product consumer product, doubtful cases shall resolved favor coverage. particular product received particular user, “normally used” refers typical common use class product, regardless status particular user way particular user actually uses, expects expected use, product. product consumer product regardless whether product substantial commercial, industrial non-consumer uses, unless uses represent significant mode use product. “Installation Information” User Product means methods, procedures, authorization keys, information required install execute modified versions covered work User Product modified version Corresponding Source. information must suffice ensure continued functioning modified object code case prevented interfered solely modification made. convey object code work section , , specifically use , User Product, conveying occurs part transaction right possession use User Product transferred recipient perpetuity fixed term (regardless transaction characterized), Corresponding Source conveyed section must accompanied Installation Information. requirement apply neither third party retains ability install modified object code User Product (example, work installed ROM). requirement provide Installation Information include requirement continue provide support service, warranty, updates work modified installed recipient, User Product modified installed. Access network may denied modification materially adversely affects operation network violates rules protocols communication across network. Corresponding Source conveyed, Installation Information provided, accord section must format publicly documented (implementation available public source code form), must require special password key unpacking, reading copying.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/LICENSE.html","id":"id_7-additional-terms","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"7. Additional Terms.","title":"GNU Affero General Public License","text":"“Additional permissions” terms supplement terms License making exceptions one conditions. Additional permissions applicable entire Program shall treated though included License, extent valid applicable law. additional permissions apply part Program, part may used separately permissions, entire Program remains governed License without regard additional permissions. convey copy covered work, may option remove additional permissions copy, part . (Additional permissions may written require removal certain cases modify work.) may place additional permissions material, added covered work, can give appropriate copyright permission. Notwithstanding provision License, material add covered work, may (authorized copyright holders material) supplement terms License terms: Disclaiming warranty limiting liability differently terms sections 15 16 License; Requiring preservation specified reasonable legal notices author attributions material Appropriate Legal Notices displayed works containing ; Prohibiting misrepresentation origin material, requiring modified versions material marked reasonable ways different original version; Limiting use publicity purposes names licensors authors material; Declining grant rights trademark law use trade names, trademarks, service marks; Requiring indemnification licensors authors material anyone conveys material (modified versions ) contractual assumptions liability recipient, liability contractual assumptions directly impose licensors authors. non-permissive additional terms considered “restrictions” within meaning section 10. Program received , part , contains notice stating governed License along term restriction, may remove term. license document contains restriction permits relicensing conveying License, may add covered work material governed terms license document, provided restriction survive relicensing conveying. add terms covered work accord section, must place, relevant source files, statement additional terms apply files, notice indicating find applicable terms. Additional terms, permissive non-permissive, may stated form separately written license, stated exceptions; requirements apply either way.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/LICENSE.html","id":"id_8-termination","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"8. Termination.","title":"GNU Affero General Public License","text":"may propagate modify covered work except expressly provided License. attempt otherwise propagate modify void, automatically terminate rights License (including patent licenses granted third paragraph section 11). However, cease violation License, license particular copyright holder reinstated () provisionally, unless copyright holder explicitly finally terminates license, (b) permanently, copyright holder fails notify violation reasonable means prior 60 days cessation. Moreover, license particular copyright holder reinstated permanently copyright holder notifies violation reasonable means, first time received notice violation License (work) copyright holder, cure violation prior 30 days receipt notice. Termination rights section terminate licenses parties received copies rights License. rights terminated permanently reinstated, qualify receive new licenses material section 10.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/LICENSE.html","id":"id_9-acceptance-not-required-for-having-copies","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"9. Acceptance Not Required for Having Copies.","title":"GNU Affero General Public License","text":"required accept License order receive run copy Program. Ancillary propagation covered work occurring solely consequence using peer--peer transmission receive copy likewise require acceptance. However, nothing License grants permission propagate modify covered work. actions infringe copyright accept License. Therefore, modifying propagating covered work, indicate acceptance License .","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/LICENSE.html","id":"id_10-automatic-licensing-of-downstream-recipients","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"10. Automatic Licensing of Downstream Recipients.","title":"GNU Affero General Public License","text":"time convey covered work, recipient automatically receives license original licensors, run, modify propagate work, subject License. responsible enforcing compliance third parties License. “entity transaction” transaction transferring control organization, substantially assets one, subdividing organization, merging organizations. propagation covered work results entity transaction, party transaction receives copy work also receives whatever licenses work party’s predecessor interest give previous paragraph, plus right possession Corresponding Source work predecessor interest, predecessor can get reasonable efforts. may impose restrictions exercise rights granted affirmed License. example, may impose license fee, royalty, charge exercise rights granted License, may initiate litigation (including cross-claim counterclaim lawsuit) alleging patent claim infringed making, using, selling, offering sale, importing Program portion .","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/LICENSE.html","id":"id_11-patents","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"11. Patents.","title":"GNU Affero General Public License","text":"“contributor” copyright holder authorizes use License Program work Program based. work thus licensed called contributor’s “contributor version”. contributor’s “essential patent claims” patent claims owned controlled contributor, whether already acquired hereafter acquired, infringed manner, permitted License, making, using, selling contributor version, include claims infringed consequence modification contributor version. purposes definition, “control” includes right grant patent sublicenses manner consistent requirements License. contributor grants non-exclusive, worldwide, royalty-free patent license contributor’s essential patent claims, make, use, sell, offer sale, import otherwise run, modify propagate contents contributor version. following three paragraphs, “patent license” express agreement commitment, however denominated, enforce patent (express permission practice patent covenant sue patent infringement). “grant” patent license party means make agreement commitment enforce patent party. convey covered work, knowingly relying patent license, Corresponding Source work available anyone copy, free charge terms License, publicly available network server readily accessible means, must either (1) cause Corresponding Source available, (2) arrange deprive benefit patent license particular work, (3) arrange, manner consistent requirements License, extend patent license downstream recipients. “Knowingly relying” means actual knowledge , patent license, conveying covered work country, recipient’s use covered work country, infringe one identifiable patents country reason believe valid. , pursuant connection single transaction arrangement, convey, propagate procuring conveyance , covered work, grant patent license parties receiving covered work authorizing use, propagate, modify convey specific copy covered work, patent license grant automatically extended recipients covered work works based . patent license “discriminatory” include within scope coverage, prohibits exercise , conditioned non-exercise one rights specifically granted License. may convey covered work party arrangement third party business distributing software, make payment third party based extent activity conveying work, third party grants, parties receive covered work , discriminatory patent license () connection copies covered work conveyed (copies made copies), (b) primarily connection specific products compilations contain covered work, unless entered arrangement, patent license granted, prior 28 March 2007. Nothing License shall construed excluding limiting implied license defenses infringement may otherwise available applicable patent law.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/LICENSE.html","id":"id_12-no-surrender-of-others-freedom","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"12. No Surrender of Others’ Freedom.","title":"GNU Affero General Public License","text":"conditions imposed (whether court order, agreement otherwise) contradict conditions License, excuse conditions License. convey covered work satisfy simultaneously obligations License pertinent obligations, consequence may convey . example, agree terms obligate collect royalty conveying convey Program, way satisfy terms License refrain entirely conveying Program.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/LICENSE.html","id":"id_13-remote-network-interaction-use-with-the-gnu-general-public-license","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"13. Remote Network Interaction; Use with the GNU General Public License.","title":"GNU Affero General Public License","text":"Notwithstanding provision License, modify Program, modified version must prominently offer users interacting remotely computer network (version supports interaction) opportunity receive Corresponding Source version providing access Corresponding Source network server charge, standard customary means facilitating copying software. Corresponding Source shall include Corresponding Source work covered version 3 GNU General Public License incorporated pursuant following paragraph. Notwithstanding provision License, permission link combine covered work work licensed version 3 GNU General Public License single combined work, convey resulting work. terms License continue apply part covered work, work combined remain governed version 3 GNU General Public License.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/LICENSE.html","id":"id_14-revised-versions-of-this-license","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"14. Revised Versions of this License.","title":"GNU Affero General Public License","text":"Free Software Foundation may publish revised /new versions GNU Affero General Public License time time. new versions similar spirit present version, may differ detail address new problems concerns. version given distinguishing version number. Program specifies certain numbered version GNU Affero General Public License “later version” applies , option following terms conditions either numbered version later version published Free Software Foundation. Program specify version number GNU Affero General Public License, may choose version ever published Free Software Foundation. Program specifies proxy can decide future versions GNU Affero General Public License can used, proxy’s public statement acceptance version permanently authorizes choose version Program. Later license versions may give additional different permissions. However, additional obligations imposed author copyright holder result choosing follow later version.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/LICENSE.html","id":"id_15-disclaimer-of-warranty","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"15. Disclaimer of Warranty.","title":"GNU Affero General Public License","text":"WARRANTY PROGRAM, EXTENT PERMITTED APPLICABLE LAW. EXCEPT OTHERWISE STATED WRITING COPYRIGHT HOLDERS /PARTIES PROVIDE PROGRAM “” WITHOUT WARRANTY KIND, EITHER EXPRESSED IMPLIED, INCLUDING, LIMITED , IMPLIED WARRANTIES MERCHANTABILITY FITNESS PARTICULAR PURPOSE. ENTIRE RISK QUALITY PERFORMANCE PROGRAM . PROGRAM PROVE DEFECTIVE, ASSUME COST NECESSARY SERVICING, REPAIR CORRECTION.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/LICENSE.html","id":"id_16-limitation-of-liability","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"16. Limitation of Liability.","title":"GNU Affero General Public License","text":"EVENT UNLESS REQUIRED APPLICABLE LAW AGREED WRITING COPYRIGHT HOLDER, PARTY MODIFIES /CONVEYS PROGRAM PERMITTED , LIABLE DAMAGES, INCLUDING GENERAL, SPECIAL, INCIDENTAL CONSEQUENTIAL DAMAGES ARISING USE INABILITY USE PROGRAM (INCLUDING LIMITED LOSS DATA DATA RENDERED INACCURATE LOSSES SUSTAINED THIRD PARTIES FAILURE PROGRAM OPERATE PROGRAMS), EVEN HOLDER PARTY ADVISED POSSIBILITY DAMAGES.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/LICENSE.html","id":"id_17-interpretation-of-sections-15-and-16","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"17. Interpretation of Sections 15 and 16.","title":"GNU Affero General Public License","text":"disclaimer warranty limitation liability provided given local legal effect according terms, reviewing courts shall apply local law closely approximates absolute waiver civil liability connection Program, unless warranty assumption liability accompanies copy Program return fee. END TERMS CONDITIONS","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/LICENSE.html","id":"how-to-apply-these-terms-to-your-new-programs","dir":"","previous_headings":"","what":"How to Apply These Terms to Your New Programs","title":"GNU Affero General Public License","text":"develop new program, want greatest possible use public, best way achieve make free software everyone can redistribute change terms. , attach following notices program. safest attach start source file effectively state exclusion warranty; file least “copyright” line pointer full notice found. Also add information contact electronic paper mail. software can interact users remotely computer network, also make sure provides way users get source. example, program web application, interface display “Source” link leads users archive code. many ways offer source, different solutions better different programs; see section 13 specific requirements. also get employer (work programmer) school, , sign “copyright disclaimer” program, necessary. information , apply follow GNU AGPL, see https://www.gnu.org/licenses/.","code":"<one line to give the program's name and a brief idea of what it does.>     Copyright (C) <year>  <name of author>      This program is free software: you can redistribute it and/or modify     it under the terms of the GNU Affero General Public License as     published by the Free Software Foundation, either version 3 of the     License, or (at your option) any later version.      This program is distributed in the hope that it will be useful,     but WITHOUT ANY WARRANTY; without even the implied warranty of     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the     GNU Affero General Public License for more details.      You should have received a copy of the GNU Affero General Public License     along with this program.  If not, see <https://www.gnu.org/licenses/>."},{"path":"https://evolecolgroup.github.io/tidysdm/dev/articles/a0_tidysdm_overview.html","id":"sdms-with-tidymodels","dir":"Articles","previous_headings":"","what":"SDMs with tidymodels","title":"tidysdm overview","text":"Species Distribution Modelling relies several algorithms, many number hyperparameters require turning. tidymodels universe includes number packages specifically design fit, tune validate models. advantage tidymodels models syntax results returned users standardised, thus providing coherent interface modelling. Given variety models required SDM, tidymodels ideal framework. tidysdm provides number wrappers specialised functions facilitate fitting SDM tidymodels. load tidysdm, automatically loads tidymodels associated packages necessary fit models:","code":"library(tidysdm) #> Loading required package: tidymodels #> ── Attaching packages ────────────────────────────────────── tidymodels 1.1.0 ── #> ✔ broom        1.0.5     ✔ recipes      1.0.6 #> ✔ dials        1.2.0     ✔ rsample      1.1.1 #> ✔ dplyr        1.1.2     ✔ tibble       3.2.1 #> ✔ ggplot2      3.4.2     ✔ tidyr        1.3.0 #> ✔ infer        1.0.4     ✔ tune         1.1.1 #> ✔ modeldata    1.1.0     ✔ workflows    1.1.3 #> ✔ parsnip      1.1.0     ✔ workflowsets 1.0.1 #> ✔ purrr        1.0.1     ✔ yardstick    1.2.0 #> ── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ── #> ✖ purrr::discard() masks scales::discard() #> ✖ dplyr::filter()  masks stats::filter() #> ✖ dplyr::lag()     masks stats::lag() #> ✖ recipes::step()  masks stats::step() #> • Use tidymodels_prefer() to resolve common conflicts. #> Loading required package: spatialsample #> Registered S3 methods overwritten by 'tidysdm': #>   method      from    #>   bake.recipe recipes #>   prep.recipe recipes"},{"path":"https://evolecolgroup.github.io/tidysdm/dev/articles/a0_tidysdm_overview.html","id":"preparing-your-data","dir":"Articles","previous_headings":"","what":"Preparing your data","title":"tidysdm overview","text":"start reading set presences species lizard inhabits Iberian peninsula, Lacerta schreiberi. First, let us visualise presences plotting map. tidysdm works sf objects represent locations, cast coordinates sf object, set projections standard lonlat (crs = 4326). usually advisable plot locations directly raster used extract climatic variables, see locations fall within discrete space raster. vignette, use WordClim source climatic information. access WorldClim data via library pastclim; even though library, name suggests, mostly designed handle palaeoclimatic reconstructions, also provides convenient functions access present day reconstructions future projections. pastclim handy function get land mask available datasets, can use background locations. cut raster Iberian peninsula, lizard lives. simply illustration, bother project raster, equal area projection desirable… plotting, take advantage tidyterra, makes handling terra rasters ggplot breeze.","code":"data(lacerta) lacerta #> # A tibble: 1,297 × 3 #>           ID latitude longitude #>        <dbl>    <dbl>     <dbl> #>  1 858029749     42.6     -7.09 #>  2 858029738     42.6     -7.09 #>  3 614631090     41.4     -7.90 #>  4 614631085     41.3     -7.81 #>  5 614631083     41.3     -7.81 #>  6 614631080     41.4     -7.83 #>  7 614631072     41.4     -7.81 #>  8 614559731     40.3     -7.70 #>  9 614559728     40.4     -7.70 #> 10 614559657     40.4     -7.56 #> # ℹ 1,287 more rows library(sf) #> Linking to GEOS 3.10.2, GDAL 3.4.1, PROJ 8.2.1; sf_use_s2() is TRUE lacerta <- st_as_sf(lacerta, coords = c(\"longitude\",\"latitude\")) st_crs(lacerta) = 4326 library(pastclim) land_mask <-   get_land_mask(time_ce = 1985, dataset = \"WorldClim_2.1_10m\")  # Iberia peninsula extension iberia_poly <-   terra::vect(     \"POLYGON((-9.8 43.3,-7.8 44.1,-2.0 43.7,3.6 42.5,3.8 41.5,1.3 40.8,0.3 39.5,      0.9 38.6,-0.4 37.5,-1.6 36.7,-2.3 36.3,-4.1 36.4,-4.5 36.4,-5.0 36.1,     -5.6 36.0,-6.3 36.0,-7.1 36.9,-9.5 36.6,-9.4 38.0,-10.6 38.9,-9.5 40.8,     -9.8 43.3))\"   )  crs(iberia_poly) <- \"lonlat\" # crop the extent land_mask <- crop(land_mask, iberia_poly) # and mask to the polygon land_mask <- mask(land_mask, iberia_poly) #> Loading required package: terra #> terra 1.7.41 #>  #> Attaching package: 'terra' #> The following object is masked from 'package:tidyr': #>  #>     extract #> The following object is masked from 'package:scales': #>  #>     rescale library(tidyterra) #>  #> Attaching package: 'tidyterra' #> The following object is masked from 'package:stats': #>  #>     filter library(ggplot2) ggplot() +   geom_spatraster(data=land_mask, aes(fill=land_mask_1985))+   geom_sf(data = lacerta)"},{"path":"https://evolecolgroup.github.io/tidysdm/dev/articles/a0_tidysdm_overview.html","id":"thinning-step","dir":"Articles","previous_headings":"","what":"Thinning step","title":"tidysdm overview","text":"Now, thin observations one per cell raster (better equal area projection…):  Now, thin remove points closer 20km. However, note standard map units ‘lonlat’ projection meters. tidysdm provides convening conversion function, km2m(), avoid write lots zeroes): Let’s see left points:  Now sample pseudo-absences (constrain least 50km away presences), selecting 3 times many points presences: Let’s see presences absences:  Generally, can use pastclim check variables available WordClim dataset: first download dataset right resolution (10 arc-minutes): create terra SpatRaster object. dataset covers period 1970-2000, pastclim dates 1985 (midpoint). can directly crop Iberian peninsula: start selecting variables presences markedly different underlying background. First, let’s extract climate presences pseudo-absences: can use violin plots compare distribution climatic variables presences pseudo-absences: want choose variables presences use values different background (pseudo-absences). can qualitatively look plots, use quantitative approach ranks based overlap respective density plots: first step, can focus variables least 30% non-overlapping distribution presences pseudo-absences: still lot variables. Among , several argued past important distribution species interest. Based paper (https://doi.org/10.1007/s10531-010-9865-2), interested : Environmental variables often highly correlated, collinearity issue several types models. can inspect correlation among variables :  can see variables rather high correlation (e.g. bio05 vs bio14). can subset variables certain threshold correlation (e.g. 0.7) : , removing bio14 leaves us set uncorrelated variables.","code":"set.seed(1234567) lacerta<-thin_by_cell(lacerta, raster = land_mask) nrow(lacerta) #> [1] 231 ggplot() +   geom_spatraster(data=land_mask, aes(fill=land_mask_1985))+   geom_sf(data = lacerta) set.seed(1234567) lacerta_thin<-thin_by_dist(lacerta, dist_min = km2m(20)) nrow(lacerta_thin) #> [1] 113 ggplot() +   geom_spatraster(data=land_mask, aes(fill=land_mask_1985))+   geom_sf(data = lacerta_thin) set.seed(1234567) lacerta_thin <- sample_pseudoabs(lacerta_thin,                                 n = 3 * nrow(lacerta_thin),                                 raster = land_mask,                                method = c(\"dist_min\", km2m(50))) ggplot() +   geom_spatraster(data=land_mask, aes(fill=land_mask_1985))+   geom_sf(data = lacerta_thin, aes(col = class)) climate_vars <- get_vars_for_dataset(\"WorldClim_2.1_10m\") download_dataset(\"WorldClim_2.1_10m\") climate_present<-pastclim::region_slice(time_ce = 1985,                                          bio_variables = climate_vars,                                          data=\"WorldClim_2.1_10m\",                                          crop=iberia_poly) lacerta_thin <- lacerta_thin %>%    bind_cols(terra::extract(climate_present, lacerta_thin, ID=FALSE)) lacerta_thin %>% plot_pres_vs_bg(class) lacerta_thin %>% dist_pres_vs_bg(class) #>     bio19     bio12     bio16     bio02     bio13     bio07     bio05     bio04  #> 0.6935026 0.6929654 0.6885097 0.6776561 0.6679894 0.6542063 0.6243068 0.6071527  #>     bio09     bio10     bio17     bio15     bio18     bio06     bio08     bio14  #> 0.5506741 0.4855064 0.4318872 0.4241999 0.3600587 0.3454245 0.3108560 0.2954351  #>     bio11     bio03     bio01  altitude  #> 0.2317079 0.2173934 0.2045136 0.1554400 vars_to_keep <- lacerta_thin %>% dist_pres_vs_bg(class) vars_to_keep <-names(vars_to_keep[vars_to_keep>0.30]) lacerta_thin <-lacerta_thin %>% select(all_of(c(vars_to_keep, \"class\"))) vars_to_keep #>  [1] \"bio19\" \"bio12\" \"bio16\" \"bio02\" \"bio13\" \"bio07\" \"bio05\" \"bio04\" \"bio09\" #> [10] \"bio10\" \"bio17\" \"bio15\" \"bio18\" \"bio06\" \"bio08\" suggested_vars <- c(\"bio05\",\"bio06\", \"bio13\", \"bio14\", \"bio15\") pairs(climate_present[[suggested_vars]]) climate_present<-climate_present[[suggested_vars]] vars_uncor <- filter_high_cor(climate_present, cutoff = 0.7) vars_uncor #> [1] \"bio15\" \"bio05\" \"bio13\" \"bio06\" #> attr(,\"to_remove\") #> [1] \"bio14\" lacerta_thin <-lacerta_thin %>% select(all_of(c(vars_uncor, \"class\"))) climate_present<-climate_present[[vars_uncor]]"},{"path":"https://evolecolgroup.github.io/tidysdm/dev/articles/a0_tidysdm_overview.html","id":"fit-the-model-by-cross-validation","dir":"Articles","previous_headings":"","what":"Fit the model by cross-validation","title":"tidysdm overview","text":"Next, need set recipe define handle dataset. don’t want anything data terms transformations, just need define formula (class outcome, variables predictors; note , sf objects, geometry automatically ignored predictor): classification models tidymodels, assumption level interest response (case, presences) reference level. can confirm data correctly formatted : now build workflow_set different models, defining hyperparameters want tune. use glm, random forest, boosted_trees maxent models. latter three tunable hyperparameters. commonly used models, tidysdm automatically chooses important parameters, possible fully customise model specifications. now want set spatial block cross-validation scheme tune assess models. 80:20 split, creating 5 folds.  can now use block CV folds tune assess models (keep computations fast, explore 3 combination hypeparameters per model; far little real life!): Note workflow_set correctly detects tuning parameters glm. can look performance models : Now let’s create ensemble, selecting best set parameters model (really relevant random forest, hype-parameters tune glm gam). use Boyce continuous index metric choose best random forest boosted tree. adding members ensemble, automatically fitted full training dataset, ready make predictions. visualise ","code":"lacerta_rec <- recipe(lacerta_thin, formula=class~.) lacerta_rec #>  #> ── Recipe ────────────────────────────────────────────────────────────────────── #>  #> ── Inputs #> Number of variables by role #> outcome:   1 #> predictor: 4 lacerta_thin %>% check_sdm_presence(class) #> [1] TRUE lacerta_models <-   # create the workflow_set   workflow_set(     preproc = list(default = lacerta_rec),     models = list(       # the standard glm specs       glm = sdm_spec_glm(),       # rf specs with tuning       rf = sdm_spec_rf(),       # boosted tree model (gbm) specs with tuning       gbm = sdm_spec_boost_tree(),       # maxent specs with tuning       maxent =sdm_spec_maxent()     ),     # make all combinations of preproc and models,     cross = TRUE   ) %>%   # tweak controls to store information needed later to create the ensemble   option_add(control = control_ensemble_grid()) library(tidysdm) set.seed(100) lacerta_cv <- spatial_block_cv(lacerta_thin, v = 5) autoplot(lacerta_cv) set.seed(1234567) lacerta_models <-     lacerta_models %>%     workflow_map(\"tune_grid\", resamples = lacerta_cv, grid = 3,                  metrics = sdm_metric_set(), verbose = TRUE) #> i  No tuning parameters. `fit_resamples()` will be attempted #> i 1 of 4 resampling: default_glm #> ✔ 1 of 4 resampling: default_glm (716ms) #> i 2 of 4 tuning:     default_rf #> i Creating pre-processing data to finalize unknown parameter: mtry #> ✔ 2 of 4 tuning:     default_rf (2.5s) #> i 3 of 4 tuning:     default_gbm #> i Creating pre-processing data to finalize unknown parameter: mtry #> ✔ 3 of 4 tuning:     default_gbm (12.9s) #> i 4 of 4 tuning:     default_maxent #> ✔ 4 of 4 tuning:     default_maxent (4.6s) autoplot(lacerta_models) lacerta_ensemble <- simple_ensemble() %>%   add_member(lacerta_models, metric=\"boyce_cont\") lacerta_ensemble #> A simple_ensemble of models #>  #> Members: #> • default_glm #> • default_rf #> • default_gbm #> • default_maxent #>  #> Available metrics: #> • boyce_cont #> • roc_auc #> • tss_max #>  #> Metric used to tune workflows: #> • boyce_cont autoplot(lacerta_ensemble)"},{"path":"https://evolecolgroup.github.io/tidysdm/dev/articles/a0_tidysdm_overview.html","id":"projecting-to-the-present","dir":"Articles","previous_headings":"","what":"Projecting to the present","title":"tidysdm overview","text":"can now make predictions ensemble (using default option taking mean predictions model).  can subset ensemble use best models, based Boyce continuous index, setting minimum threshold 0.8 metric. also take median available model predictions (instead mean, default). plot change much (models quite consistent). Sometimes, desirable binary predictions (presence vs absence), rather probability occurrence. , first need calibrate threshold used convert probabilities classes (case, optimise TSS): now can predict whole continent:","code":"prediction_present <- predict_raster(lacerta_ensemble, climate_present) ggplot() +   geom_spatraster(data=prediction_present, aes(fill=mean))+   scale_fill_terrain_c() +   # plot presences used in the model   geom_sf(data = lacerta_thin %>% filter(class==\"presence\")) prediction_present_boyce <- predict_raster(lacerta_ensemble, climate_present,                                            metric_thresh = c(\"boyce_cont\", 0.8),                                            fun=\"median\") ggplot() +   geom_spatraster(data=prediction_present_boyce, aes(fill=median))+   scale_fill_terrain_c() +   geom_sf(data = lacerta_thin %>% filter(class==\"presence\")) lacerta_ensemble<-calib_class_thresh(lacerta_ensemble,                                          class_thresh = \"tss_max\") prediction_present_binary <- predict_raster(lacerta_ensemble,                                              climate_present,                                             type=\"class\",                                             class_thresh = c(\"tss_max\")) ggplot() +   geom_spatraster(data=prediction_present_binary, aes(fill=binary_mean))+   geom_sf(data = lacerta_thin %>% filter(class==\"presence\"))"},{"path":"https://evolecolgroup.github.io/tidysdm/dev/articles/a0_tidysdm_overview.html","id":"projecting-to-the-future","dir":"Articles","previous_headings":"","what":"Projecting to the future","title":"tidysdm overview","text":"WorlClim wide selection projections future based different models Shared Socio-economic Pathways (SSP). Type help(\"WorldClim_2.1\") full list. use predictions based “HadGEM3-GC31-LL” model SSP 245 (intermediate green house gas emissions) resolution present day data (10 arc-minutes). first download data: Let’s see times available: predict 2090, prediction future available. Let’s now check available variables: Note future predictions include altitude (change time), needed , copy present. However, set uncorrelated variables used earlier, don’t need worry . predict using ensemble:","code":"download_dataset(\"WorldClim_2.1_HadGEM3-GC31-LL_ssp245_10m\") get_time_ce_steps(\"WorldClim_2.1_HadGEM3-GC31-LL_ssp245_10m\") #> [1] 2030 2050 2070 2090 get_vars_for_dataset(\"WorldClim_2.1_HadGEM3-GC31-LL_ssp245_10m\") #>  [1] \"bio01\" \"bio02\" \"bio03\" \"bio04\" \"bio05\" \"bio06\" \"bio07\" \"bio08\" \"bio09\" #> [10] \"bio10\" \"bio11\" \"bio12\" \"bio13\" \"bio14\" \"bio15\" \"bio16\" \"bio17\" \"bio18\" #> [19] \"bio19\" climate_future<-pastclim::region_slice(time_ce = 2090,                                         bio_variables = vars_uncor,                                        data=\"WorldClim_2.1_HadGEM3-GC31-LL_ssp245_10m\",                                         crop=iberia_poly) prediction_future <- predict_raster(lacerta_ensemble, climate_future)  ggplot() +   geom_spatraster(data=prediction_future, aes(fill=mean))+   scale_fill_terrain_c()"},{"path":"https://evolecolgroup.github.io/tidysdm/dev/articles/a0_tidysdm_overview.html","id":"repeated-ensembles","dir":"Articles","previous_headings":"","what":"Repeated ensembles","title":"tidysdm overview","text":"steps thinning sampling pseudo-absences can bit impact performance SDMs. steps stochastic, good practice explore effect repeating , creating ensembles models repeats. tidysdm, possible create repeat_ensembles. start creating list simple_ensembles, looping SDM pipeline. just use two fast models speed process. Now can create repeat_ensemble list: can predict usual way (take mean median models):","code":"# empty object to store the simple ensembles that we will create ensemble_list <- list() for (i_repeat in 1:3){   # thin the data  lacerta_thin_rep<-thin_by_cell(lacerta, raster = climate_present)  lacerta_thin_rep<-thin_by_dist(lacerta_thin_rep, dist_min = 20000)  # sample pseudo-absences   lacerta_thin_rep <- sample_pseudoabs(lacerta_thin_rep,                                n=3*nrow(lacerta_thin_rep),                                raster=climate_present,                                method=c(\"dist_min\", 50000))   # get climate   lacerta_thin_rep <- lacerta_thin_rep %>%   bind_cols(terra::extract(climate_present, lacerta_thin_rep, ID=FALSE))  # create folds   lacerta_thin_rep_cv <- spatial_block_cv(lacerta_thin_rep, v = 5)   # create a recipe   lacerta_thin_rep_rec <- recipe(lacerta_thin_rep, formula=class~.)   # create a workflow_set   lacerta_thin_rep_models <-   # create the workflow_set   workflow_set(     preproc = list(default = lacerta_thin_rep_rec),     models = list(       # the standard glm specs       glm = sdm_spec_glm(),       # maxent specs with tuning       maxent =sdm_spec_maxent()     ),     # make all combinations of preproc and models,     cross = TRUE   ) %>%   # tweak controls to store information needed later to create the ensemble   option_add(control = control_ensemble_grid())  # train the model   lacerta_thin_rep_models <-    lacerta_thin_rep_models %>%    workflow_map(\"tune_grid\", resamples = lacerta_thin_rep_cv, grid = 10,                 metrics = sdm_metric_set(), verbose = TRUE)   # make an simple ensemble and add it to the list ensemble_list[[i_repeat]] <- simple_ensemble() %>%   add_member(lacerta_thin_rep_models, metric=\"boyce_cont\") } #> i  No tuning parameters. `fit_resamples()` will be attempted #> i 1 of 2 resampling: default_glm #> ✔ 1 of 2 resampling: default_glm (648ms) #> i 2 of 2 tuning:     default_maxent #> ✔ 2 of 2 tuning:     default_maxent (14.3s) #> i  No tuning parameters. `fit_resamples()` will be attempted #> i 1 of 2 resampling: default_glm #> ✔ 1 of 2 resampling: default_glm (639ms) #> i 2 of 2 tuning:     default_maxent #> ✔ 2 of 2 tuning:     default_maxent (14.5s) #> i  No tuning parameters. `fit_resamples()` will be attempted #> i 1 of 2 resampling: default_glm #> ✔ 1 of 2 resampling: default_glm (652ms) #> i 2 of 2 tuning:     default_maxent #> ✔ 2 of 2 tuning:     default_maxent (14.6s) lacerta_thin_rep_ens <- repeat_ensemble() %>% add_repeat(ensemble_list) lacerta_thin_rep_ens #> A repeat_ensemble of models #>  #> Number of repeats: #> • 3 #>  #> Members: #> • default_glm #> • default_maxent #>  #> Available metrics: #> • boyce_cont #> • roc_auc #> • tss_max #>  #> Metric used to tune workflows: #> • boyce_cont lacerta_thin_rep_ens <- predict_raster(lacerta_thin_rep_ens, climate_present,                                      fun=c(\"mean\",\"median\")) ggplot() +   geom_spatraster(data=lacerta_thin_rep_ens, aes(fill=median))+   scale_fill_terrain_c()"},{"path":"https://evolecolgroup.github.io/tidysdm/dev/articles/a1_palaeodata_application.html","id":"sdms-with-tidymodels-for-palaeo-data","dir":"Articles","previous_headings":"","what":"SDMs with tidymodels for palaeo data","title":"application with palaeodata","text":"vignette, show Species Distribution Model can fitted tidysdm time-scattered (.e.palaeontological, archaeozoological, archaeological) data, samples covering different time periods. first load tidysdm:","code":"library(tidysdm) #> Loading required package: tidymodels #> ── Attaching packages ────────────────────────────────────── tidymodels 1.1.0 ── #> ✔ broom        1.0.5     ✔ recipes      1.0.6 #> ✔ dials        1.2.0     ✔ rsample      1.1.1 #> ✔ dplyr        1.1.2     ✔ tibble       3.2.1 #> ✔ ggplot2      3.4.2     ✔ tidyr        1.3.0 #> ✔ infer        1.0.4     ✔ tune         1.1.1 #> ✔ modeldata    1.1.0     ✔ workflows    1.1.3 #> ✔ parsnip      1.1.0     ✔ workflowsets 1.0.1 #> ✔ purrr        1.0.1     ✔ yardstick    1.2.0 #> ── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ── #> ✖ purrr::discard() masks scales::discard() #> ✖ dplyr::filter()  masks stats::filter() #> ✖ dplyr::lag()     masks stats::lag() #> ✖ recipes::step()  masks stats::step() #> • Use tidymodels_prefer() to resolve common conflicts. #> Loading required package: spatialsample #> Registered S3 methods overwritten by 'tidysdm': #>   method      from    #>   bake.recipe recipes #>   prep.recipe recipes"},{"path":"https://evolecolgroup.github.io/tidysdm/dev/articles/a1_palaeodata_application.html","id":"preparing-your-data","dir":"Articles","previous_headings":"","what":"Preparing your data","title":"application with palaeodata","text":"start loading set radiocarbon dates (calibrated) horses, covering 22k years ago 8k years ago. convert dataset sf data.frame can easily plot (tidyterra shines): background presences, use land mask present, taken pastclim, cut cover Europe: use tidyterra plot:  now thin presences, locations 100km 2000 years apart. see left:  now need time series palaeoclimate reconstructions. vignette, use example dataset pastclim. dataset reconstructions every 5k years past 20k years 1 degree resolution, 3 bioclimatic variables. suffice illustrative purposes, recommend download higher quality datasets pastclim real analysis. land mask, cut reconstructions cover Europe : Now thin observations keep one per cell raster (better equal area projection…), remove locations outside desired area (): Let’s see left points:  Now sample pseudo-absences (constraint least 70km away presences), selecting three times number presences Let’s see presences absences:  Now let’s get climate location. pastclim requires data frame two columns coordinates column time years present (negative values represent time past). manipulate sf object accordingly:","code":"data(horses) horses #> # A tibble: 788 × 3 #>    latitude longitude time_bp #>       <dbl>     <dbl>   <int> #>  1     43.2     -2.04  -14000 #>  2     43.2     -2.04  -14000 #>  3     43.2     -2.04  -14000 #>  4     43.2     -2.04  -14000 #>  5     43.2     -2.04  -16000 #>  6     43.3     -1.89  -16000 #>  7     43.2     -2.2   -14000 #>  8     43.2     -2.2   -19000 #>  9     43.2     -2.2   -20000 #> 10     43.2     -2.2   -21000 #> # ℹ 778 more rows library(sf) #> Linking to GEOS 3.10.2, GDAL 3.4.1, PROJ 8.2.1; sf_use_s2() is TRUE horses <- st_as_sf(horses, coords = c(\"longitude\",\"latitude\")) st_crs(horses) = 4326 #> Loading required package: terra #> terra 1.7.41 #>  #> Attaching package: 'terra' #> The following object is masked from 'package:tidyr': #>  #>     extract #> The following object is masked from 'package:scales': #>  #>     rescale library(pastclim) land_mask <- pastclim::get_land_mask(time_bp=0,dataset=\"Example\") europe_poly <- vect(region_outline$Europe) crs(europe_poly) <- \"lonlat\" land_mask <- crop(land_mask, europe_poly) land_mask <- mask(land_mask, europe_poly) library(tidyterra) #>  #> Attaching package: 'tidyterra' #> The following object is masked from 'package:stats': #>  #>     filter ggplot() +   geom_spatraster(data=land_mask, aes(fill=land_mask_0))+   scale_fill_terrain_c() +   geom_sf(data = horses, aes(col=time_bp)) set.seed(123) horses<-thin_by_dist_time(horses, dist_min = km2m(100),                           interval_min = y2d(2000),                           time_col=\"time_bp\",                           lubridate_fun = ybp2date) nrow(horses) #> [1] 185 ggplot() +   geom_spatraster(data=land_mask, aes(fill=land_mask_0))+   scale_fill_terrain_c() +   geom_sf(data = horses, aes(col=time_bp)) library(pastclim) climate_vars <-c(\"bio01\", \"bio10\", \"bio12\") climate_full<-pastclim::region_series(bio_variables = climate_vars,                                        data=\"Example\",                                        crop= region_outline$Europe) set.seed(123) horses<-thin_by_cell_time(horses, raster = climate_full,                           time_col=\"time_bp\",                           lubridate_fun = ybp2date) nrow(horses) #> [1] 138 ggplot() +   geom_spatraster(data=land_mask, aes(fill=land_mask_0))+   scale_fill_terrain_c() +   geom_sf(data = horses, aes(col=time_bp)) set.seed(123) horses <- sample_pseudoabs_time(horses,                                  n_per_presence=3,                                  raster=climate_full,                                 time_col=\"time_bp\",                                 lubridate_fun = ybp2date,                                 method=c(\"dist_min\", km2m(70)) ) ggplot() +   geom_spatraster(data=land_mask, aes(fill=land_mask_0))+   scale_fill_terrain_c() +   geom_sf(data = horses, aes(col = class)) horses_df <- horses %>%   dplyr::bind_cols(sf::st_coordinates(horses)) %>%   mutate (time_bp=date2ybp(time_step)) %>%   as.data.frame() %>%    select(-geometry) # get climate horses_df <- location_slice_from_region_series(horses_df,                                                 region_series = climate_full)  # add the climate reconstructions to the sf object, and remove the time_step # as we don't need it for modelling horses <- horses %>%   bind_cols(horses_df[,climate_vars]) %>%   select(-time_step)"},{"path":"https://evolecolgroup.github.io/tidysdm/dev/articles/a1_palaeodata_application.html","id":"fit-the-model-by-crossvalidation","dir":"Articles","previous_headings":"","what":"Fit the model by crossvalidation","title":"application with palaeodata","text":"Next, need set recipe define handle dataset. don’t want transform data, just need define formula (class outcome, variables predictors; note , sf objects, geometry automatically ignored predictor): can quickly check variables want : now build workflow_set different models, defining hyperparameters want tune. use glm, gam, random forest boosted trees models, random forest boosted trees tunable hyperparameters. commonly used models, tidysdm automatically chooses important parameters, possible fully customise model specifications. Note gams unusual, need specify formula define variables fit smooths. default, gam_formula() fits smooth every continuous predictor, custom formula can provided instead. now want set spatial block cross-validation scheme tune assess models:  can now use block CV folds tune assess models: Note workflow_set correctly detects tuning parameters glm gam. can look performance models :  Now let’s create ensemble, selecting best set parameters model (really relevant random forest, hype-parameters tune glm gam). use Boyce continuous index metric choose best random forest boosted tree. adding members ensemble, automatically fitted full training dataset, ready make predictions. visualise ","code":"horses_rec <- recipe(horses, formula=class~.) horses_rec #>  #> ── Recipe ────────────────────────────────────────────────────────────────────── #>  #> ── Inputs #> Number of variables by role #> outcome:   1 #> predictor: 3 horses_rec$var_info #> # A tibble: 4 × 4 #>   variable type      role      source   #>   <chr>    <list>    <chr>     <chr>    #> 1 bio01    <chr [2]> predictor original #> 2 bio10    <chr [2]> predictor original #> 3 bio12    <chr [2]> predictor original #> 4 class    <chr [3]> outcome   original horses_models <-   # create the workflow_set   workflow_set(     preproc = list(default = horses_rec),     models = list(       # the standard glm specs  (no params to tune)       glm = sdm_spec_glm(),       # the standard sdm specs (no params to tune)       gam = sdm_spec_gam(),       # rf specs with tuning       rf = sdm_spec_rf(),       # boosted tree model (gbm) specs with tuning       gbm = sdm_spec_boost_tree()     ),     # make all combinations of preproc and models,     cross = TRUE   ) %>%   # set formula for gams   update_workflow_model(\"default_gam\",                         spec = sdm_spec_gam(),                         formula = gam_formula(horses_rec)) %>%   # tweak controls to store information needed later to create the ensemble   option_add(control = control_ensemble_grid()) library(tidysdm) set.seed(1005) horses_cv <- spatial_block_cv(horses, v = 5) autoplot(horses_cv) set.seed(123) horses_models <-    horses_models %>%    workflow_map(\"tune_grid\", resamples = horses_cv, grid = 5,                 metrics = sdm_metric_set(), verbose = TRUE) #> i  No tuning parameters. `fit_resamples()` will be attempted #> i 1 of 4 resampling: default_glm #> ✔ 1 of 4 resampling: default_glm (688ms) #> i  No tuning parameters. `fit_resamples()` will be attempted #> i 2 of 4 resampling: default_gam #> ✔ 2 of 4 resampling: default_gam (1.2s) #> i 3 of 4 tuning:     default_rf #> i Creating pre-processing data to finalize unknown parameter: mtry #> ✔ 3 of 4 tuning:     default_rf (4.7s) #> i 4 of 4 tuning:     default_gbm #> i Creating pre-processing data to finalize unknown parameter: mtry #> ✔ 4 of 4 tuning:     default_gbm (31.5s) autoplot(horses_models) horses_ensemble <- simple_ensemble() %>%   add_member(horses_models, metric=\"boyce_cont\") autoplot(horses_ensemble)"},{"path":"https://evolecolgroup.github.io/tidysdm/dev/articles/a1_palaeodata_application.html","id":"projecting-to-other-times","dir":"Articles","previous_headings":"","what":"Projecting to other times","title":"application with palaeodata","text":"can now make predictions ensemble (using default option taking mean predictions model) Last Glacial Maximum (LGM, 21,000 years ago). predict using ensemble:","code":"climate_lgm <- pastclim::region_slice(   time_bp = -20000,   bio_variables = climate_vars,   data = \"Example\",   crop = region_outline$Europe ) prediction_lgm <- predict_raster(horses_ensemble, climate_lgm) ggplot() +   geom_spatraster(data = prediction_lgm, aes(fill = mean)) +   scale_fill_terrain_c()"},{"path":"https://evolecolgroup.github.io/tidysdm/dev/articles/a2_tidymodels_additions.html","id":"additional-features-of-tidymodels","dir":"Articles","previous_headings":"","what":"Additional features of tidymodels","title":"Example of additional tidymodels features","text":"vignette, illustrate number features tidymodels can used enhance conventional SDM pipeline. recommend users first become familiar tidymodels; number excellent tutorials dedicated website reuse example Iberian lizard used give overview features tidysdm.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/articles/a2_tidymodels_additions.html","id":"preparing-your-data","dir":"Articles","previous_headings":"","what":"Preparing your data","title":"Example of additional tidymodels features","text":"quickly shape dataset overview; see vignette detailed explantion steps. start reading set presences species lizard inhabits Iberian peninsula, Lacerta schreiberi, cast sf object Get appropriate land mask thin presences (reset seed operation match original vignette): Quickly plot :  Let’s extract climate variables interest","code":"library(tidysdm) #> Loading required package: tidymodels #> ── Attaching packages ────────────────────────────────────── tidymodels 1.1.0 ── #> ✔ broom        1.0.5     ✔ recipes      1.0.6 #> ✔ dials        1.2.0     ✔ rsample      1.1.1 #> ✔ dplyr        1.1.2     ✔ tibble       3.2.1 #> ✔ ggplot2      3.4.2     ✔ tidyr        1.3.0 #> ✔ infer        1.0.4     ✔ tune         1.1.1 #> ✔ modeldata    1.1.0     ✔ workflows    1.1.3 #> ✔ parsnip      1.1.0     ✔ workflowsets 1.0.1 #> ✔ purrr        1.0.1     ✔ yardstick    1.2.0 #> ── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ── #> ✖ purrr::discard() masks scales::discard() #> ✖ dplyr::filter()  masks stats::filter() #> ✖ dplyr::lag()     masks stats::lag() #> ✖ recipes::step()  masks stats::step() #> • Dig deeper into tidy modeling with R at https://www.tmwr.org #> Loading required package: spatialsample #> Registered S3 methods overwritten by 'tidysdm': #>   method      from    #>   bake.recipe recipes #>   prep.recipe recipes data(lacerta) library(sf) #> Linking to GEOS 3.10.2, GDAL 3.4.1, PROJ 8.2.1; sf_use_s2() is TRUE lacerta <- st_as_sf(lacerta, coords = c(\"longitude\",\"latitude\")) st_crs(lacerta) = 4326 library(pastclim) land_mask <-   get_land_mask(time_ce = 1985, dataset = \"WorldClim_2.1_10m\")  # Iberia peninsula extension iberia_poly <-   terra::vect(     \"POLYGON((-9.8 43.3,-7.8 44.1,-2.0 43.7,3.6 42.5,3.8 41.5,1.3 40.8,0.3 39.5,      0.9 38.6,-0.4 37.5,-1.6 36.7,-2.3 36.3,-4.1 36.4,-4.5 36.4,-5.0 36.1,     -5.6 36.0,-6.3 36.0,-7.1 36.9,-9.5 36.6,-9.4 38.0,-10.6 38.9,-9.5 40.8,     -9.8 43.3))\"   )  crs(iberia_poly) <- \"lonlat\" # crop the extent land_mask <- crop(land_mask, iberia_poly) # and mask to the polygon land_mask <- mask(land_mask, iberia_poly) #> Loading required package: terra #> terra 1.7.41 #>  #> Attaching package: 'terra' #> The following object is masked from 'package:tidyr': #>  #>     extract #> The following object is masked from 'package:scales': #>  #>     rescale set.seed(1234567) lacerta<-thin_by_cell(lacerta, raster = land_mask) set.seed(1234567) lacerta_thin<-thin_by_dist(lacerta, dist_min = km2m(20)) set.seed(1234567) lacerta_thin <- sample_pseudoabs(lacerta_thin,                                 n = 3 * nrow(lacerta_thin),                                 raster = land_mask,                                method = c(\"dist_min\", km2m(50))) library(tidyterra) #>  #> Attaching package: 'tidyterra' #> The following object is masked from 'package:stats': #>  #>     filter ggplot() +   geom_spatraster(data=land_mask, aes(fill=land_mask_1985))+   geom_sf(data = lacerta_thin, aes(col = class)) download_dataset(\"WorldClim_2.1_10m\") climate_present<-pastclim::region_slice(time_ce = 1985,                                          bio_variables = climate_vars,                                          data=\"WorldClim_2.1_10m\",                                          crop=iberia_poly) lacerta_thin <- lacerta_thin %>%    bind_cols(terra::extract(climate_present, lacerta_thin, ID=FALSE))"},{"path":"https://evolecolgroup.github.io/tidysdm/dev/articles/a2_tidymodels_additions.html","id":"the-initial-split","dir":"Articles","previous_headings":"","what":"The initial split","title":"Example of additional tidymodels features","text":"standard approach tidymodels make initial split data test training set. use retain 20% data (1/5) testing set, use rest training.","code":"set.seed(1005) lacerta_initial <- spatial_initial_split(lacerta_thin, prop = 1/5, spatial_block_cv) autoplot(lacerta_initial)"},{"path":"https://evolecolgroup.github.io/tidysdm/dev/articles/a2_tidymodels_additions.html","id":"fit-the-model-to-the-training-set-tuning-by-crossvalidation","dir":"Articles","previous_headings":"","what":"Fit the model to the training set, tuning by crossvalidation","title":"Example of additional tidymodels features","text":"can now extract training set lacerta_initial split, sample folds set crossvalidation (use grid used full dataset arabiensis initial_split)  certain type models (e.g. glm, mars) struggle correlated variables; algorithms, random forests, can handle correlated variables. , create two recipes, one variables, one variables uncorrelated: now use workflowset (keep small computational time), selecting appropriate recipe model. include model (multivariate adaptive regression splines, MARS) wrapper tidysdm creating model specificaiton. However, can use standard model spec yardstick: can now use block CV folds tune assess models. Note multiple tuning approaches, besides standard grid method. use tune_bayes (keep computations fast, explore 3 combination hypeparameters per model; far little real life!). tuning method (opposed use standard grid) allow hyperparameters unknown limits, mtry random forest gbm undefined upper range depends number variables dataset. , tuning, need finalise mtry informing set dials actual data: now can tune models: can look performance models :  Instead building simple ensemble best version model type, can build stack ensemble, implemented package stacks. Stacking uses meta-learning algorithm learn best combine multiple models, including multiple versions algorithm different hyperparameters.  can see two versions random forest one mars selected; stacking coefficients give indication weight model carries within ensemble. can now use ensemble make predictions testing data: look goodness fit using commonly used sdm metrics. Note sdm_metric_set first invoked generate function (empty ()) used data.","code":"set.seed(1005) lacerta_training <- training(lacerta_initial) lacerta_cv <- spatial_block_cv(lacerta_training, v = 5,                                 cellsize = grid_cellsize(lacerta_thin),                                 offset = grid_offset(lacerta)) autoplot(lacerta_cv) lacerta_rec_all <- recipe(lacerta_thin, formula=class~.) lacerta_rec_uncor <- lacerta_rec_all %>% step_select(all_of(c(\"class\",\"bio05\",\"bio06\", \"bio13\", \"bio15\"))) lacerta_rec_uncor <- lacerta_rec_all %>% step_rm(all_of( c(\"bio01\", \"bio02\", \"bio03\", \"bio04\", \"bio07\", \"bio08\", \"bio09\", \"bio10\", \"bio11\", \"bio12\", \"bio14\", \"bio16\", \"bio17\", \"bio18\", \"bio19\", \"altitude\")))   lacerta_rec_uncor #>  #> ── Recipe ────────────────────────────────────────────────────────────────────── #>  #> ── Inputs #> Number of variables by role #> outcome:    1 #> predictor: 20 #>  #> ── Operations #> • Variables removed: all_of(c(\"bio01\", \"bio02\", \"bio03\", \"bio04\", \"bio07\", #>   \"bio08\", \"bio09\", \"bio10\", \"bio11\", \"bio12\", \"bio14\", \"bio16\", \"bio17\", #>   \"bio18\", \"bio19\", \"altitude\")) lacerta_models <-   # create the workflow_set   workflow_set(     preproc = list(uncor = lacerta_rec_uncor, # recipe for the glm                    all = lacerta_rec_all,  # recipe for the random forest                    all = lacerta_rec_uncor # recipe for mars     ),     models = list(       # the standard glm specs       glm = sdm_spec_glm(),       # rf specs with tuning       rf = sdm_spec_rf(),       # mars specs with tuning       mars = parsnip::mars(num_terms=tune()) %>%          parsnip::set_engine(\"earth\") %>%         parsnip::set_mode(\"classification\")     ),     # make all combinations of preproc and models,     cross = FALSE   ) %>%   # tweak controls to store information needed later to create the ensemble   # note that we use the bayes version as we will use a Bayes search (see later)   option_add(control = stacks::control_stack_bayes()) rf_param <- lacerta_models %>%   # extract the rf workflow   extract_workflow (\"all_rf\") %>%   # extract its parameters dials (used to tune)   extract_parameter_set_dials() %>%   # give it the predictors to finalize mtry   finalize(x=st_drop_geometry(lacerta_thin) %>% select(-class))  # now update the workflowset with the new parameter info lacerta_models <- lacerta_models %>%   option_add(param_info=rf_param,id=\"all_rf\") set.seed(1234567) lacerta_models <-     lacerta_models %>%     workflow_map(\"tune_bayes\", resamples = lacerta_cv, initial=8,                 metrics = sdm_metric_set(), verbose = TRUE) #> i  No tuning parameters. `fit_resamples()` will be attempted #> i 1 of 3 resampling: uncor_glm #> ✔ 1 of 3 resampling: uncor_glm (890ms) #> i 2 of 3 tuning:     all_rf #> ! No improvement for 10 iterations; returning current results. #> ✔ 2 of 3 tuning:     all_rf (31.1s) #> i 3 of 3 tuning:     all_mars #> ✔ 3 of 3 tuning:     all_mars (2.2s) autoplot(lacerta_models) library(stacks) set.seed(1005) lacerta_stack <-    # initialize the stack   stacks() %>%   # add candidate members   add_candidates(lacerta_models) %>%   # determine how to combine their predictions   blend_predictions() %>%   # fit the candidates with nonzero weights (i.e.nonzero stacking coefficients)   fit_members()  autoplot(lacerta_stack, type = \"weights\") lacerta_testing <- testing(lacerta_initial)  lacerta_test_pred <-    lacerta_testing %>%   bind_cols(predict(lacerta_stack, ., type=\"prob\")) sdm_metric_set()(data= lacerta_test_pred,truth=class,.pred_presence) #> # A tibble: 3 × 3 #>   .metric    .estimator .estimate #>   <chr>      <chr>          <dbl> #> 1 roc_auc    binary         0.993 #> 2 boyce_cont binary         0.625 #> 3 tss_max    binary         0.932"},{"path":"https://evolecolgroup.github.io/tidysdm/dev/articles/a2_tidymodels_additions.html","id":"projecting-to-the-present","dir":"Articles","previous_headings":"","what":"Projecting to the present","title":"Example of additional tidymodels features","text":"can now make predictions ensemble (using default option taking mean predictions model).","code":"prediction_present <- predict_raster(lacerta_stack, climate_present, type=\"prob\") ggplot() +   geom_spatraster(data=prediction_present, aes(fill=.pred_presence))+   scale_fill_terrain_c() +   # plot presences used in the model   geom_sf(data = lacerta_thin %>% filter(class==\"presence\"))"},{"path":"https://evolecolgroup.github.io/tidysdm/dev/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Michela Leonardi. Author. Margherita Colucci. Author. Andrea Manica. Author, maintainer.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Leonardi M, Colucci M, Manica (2023). tidysdm: Species Distribution Models tidymodels. https://github.com/EvolEcolGroup/tidysdm, https://evolecolgroup.github.io/tidysdm/.","code":"@Manual{,   title = {tidysdm: Species Distribution Models with tidymodels},   author = {Michela Leonardi and Margherita Colucci and Andrea Manica},   year = {2023},   note = {https://github.com/EvolEcolGroup/tidysdm, https://evolecolgroup.github.io/tidysdm/}, }"},{"path":"https://evolecolgroup.github.io/tidysdm/dev/index.html","id":"tidysdm-","dir":"","previous_headings":"","what":"Species Distribution Models with tidymodels","title":"Species Distribution Models with tidymodels","text":"goal tidysdm implement Species Distribution Models using tidymodels framework. advantage tidymodels model syntax results returned user standardised, thus providing coherent interface modelling. Given variety models required SDM, tidymodels ideal framework. tidysdm provides number wrappers specialised functions facilitate fitting SDM tidymodels. Besides modelling contemporary species, tidysdm number functions specifically designed work palaeontological data. Whilst users free use environmental data, articles showcase potential integration pastclim, helps downloading manipulating present day data, future predictions, palaeoclimate reconstructions.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Species Distribution Models with tidymodels","text":"tidysdm still beta stage development, official release CRAN yet. can install latest version tidysdm GitHub : development version tidysdm, includes experimental features might mature yet, use: take advantage integration pastclim highlighted articles, need dev version (one CRAN). can obtain :","code":"# install.packages(\"devtools\") devtools::install_github(\"EvolEcolGroup/tidysdm\") devtools::install_github(\"EvolEcolGroup/tidysdm\", ref = \"dev\") install.packages('terra', repos='https://rspatial.r-universe.dev')  devtools::install_github(\"EvolEcolGroup/pastclim\", ref=\"dev\")"},{"path":"https://evolecolgroup.github.io/tidysdm/dev/index.html","id":"overview-of-functionality","dir":"","previous_headings":"","what":"Overview of functionality","title":"Species Distribution Models with tidymodels","text":"dedicated website, can find Articles giving step--step overview fitting SDMs contemporary species, well equivalent tutorial using palaeontological data. also dev version site updated dev branch tidysdm (top left dev website, version number red format x.x.x.9xxx, indicating development version).","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/add_member.html","id":null,"dir":"Reference","previous_headings":"","what":"Add best member of workflow to a simple ensemble — add_member","title":"Add best member of workflow to a simple ensemble — add_member","text":"function adds member(s) simple_ensemble() object, taking best member workflow provided. possible pass individual tune_results objects tuned workflow, workflowsets::workflow_set().","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/add_member.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Add best member of workflow to a simple ensemble — add_member","text":"","code":"add_member(x, member, ...)  # S3 method for default add_member(x, member, ...)  # S3 method for tune_results add_member(x, member, metric = NULL, id = NULL, ...)  # S3 method for workflow_set add_member(x, member, metric = NULL, ...)"},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/add_member.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Add best member of workflow to a simple ensemble — add_member","text":"x simple_ensemble member(s) added member  tune_results, workflowsets::workflow_set ... used moment. metric character string (NULL) metric optimize. NULL, first metric used. id name given workflow wflow_id column.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/add_member.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Add best member of workflow to a simple ensemble — add_member","text":"simple_ensemble additional member(s)","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/add_repeat.html","id":null,"dir":"Reference","previous_headings":"","what":"Add repeat(s) to a repeated ensemble — add_repeat","title":"Add repeat(s) to a repeated ensemble — add_repeat","text":"function adds repeat(s) repeat_ensemble object, repeat simple_ensemble. repeats must contain members, selected using metric.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/add_repeat.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Add repeat(s) to a repeated ensemble — add_repeat","text":"","code":"add_repeat(x, rep, ...)  # S3 method for default add_repeat(x, rep, ...)  # S3 method for simple_ensemble add_repeat(x, rep, ...)  # S3 method for list add_repeat(x, rep, ...)"},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/add_repeat.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Add repeat(s) to a repeated ensemble — add_repeat","text":"x repeat_ensemble repeat(s) added rep repeat, single simple_ensemble, list simple_ensemble objects ... used moment.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/add_repeat.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Add repeat(s) to a repeated ensemble — add_repeat","text":"repeat_ensemble additional repeat(s)","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/autoplot.simple_ensemble.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot the results of a simple ensemble — autoplot.simple_ensemble","title":"Plot the results of a simple ensemble — autoplot.simple_ensemble","text":"autoplot() method plots performance metrics ranked using metric.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/autoplot.simple_ensemble.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot the results of a simple ensemble — autoplot.simple_ensemble","text":"","code":"# S3 method for simple_ensemble autoplot(   object,   rank_metric = NULL,   metric = NULL,   std_errs = stats::qnorm(0.95),   ... )"},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/autoplot.simple_ensemble.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot the results of a simple ensemble — autoplot.simple_ensemble","text":"object simple_ensemble whose elements results. rank_metric character string metric used rank results. none given, first metric metric set used (filtering metric option). metric character vector metrics (apart rank_metric) included visualization. std_errs number standard errors plot (standard error exists). ... options pass autoplot(). Currenly unused.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/autoplot.simple_ensemble.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot the results of a simple ensemble — autoplot.simple_ensemble","text":"ggplot object.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/autoplot.simple_ensemble.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Plot the results of a simple ensemble — autoplot.simple_ensemble","text":"function intended produce default plot visualize helpful information across possible applications simple_ensemble. sophisticated plots can produced using standard ggplot2 code plotting. x-axis workflow rank set (value one best) versus performance metric(s) y-axis. multiple metrics, facets metric, rank_metric first (provided; otherwise metric used create simple_ensemble used). multiple resamples used, confidence bounds shown result (95% confidence, default).","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/autoplot.simple_ensemble.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plot the results of a simple ensemble — autoplot.simple_ensemble","text":"","code":"# we use the two_class_example from `workflowsets`   two_class_ens <- simple_ensemble() %>%     add_member(two_class_res, metric = \"roc_auc\") #> Loading required package: Formula #> Loading required package: plotmo #> Loading required package: plotrix #>  #> Attaching package: ‘plotrix’ #> The following object is masked from ‘package:scales’: #>  #>     rescale #> Loading required package: TeachingDemos   autoplot(two_class_ens)"},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/autoplot.spatial_initial_split.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a ggplot for a spatial initial rsplit. — autoplot.spatial_initial_split","title":"Create a ggplot for a spatial initial rsplit. — autoplot.spatial_initial_split","text":"method provides good visualization method spatial initial rsplit.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/autoplot.spatial_initial_split.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a ggplot for a spatial initial rsplit. — autoplot.spatial_initial_split","text":"","code":"# S3 method for spatial_initial_split autoplot(object, ..., alpha = 0.6)"},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/autoplot.spatial_initial_split.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a ggplot for a spatial initial rsplit. — autoplot.spatial_initial_split","text":"object spatial_initial_rsplit object. Note resamples made sf objects  create spatial_initial_rsplit objects; function work resamples made non-spatial tibbles data.frames. ... Options passed ggplot2::geom_sf(). alpha Opacity, passed ggplot2::geom_sf(). Values alpha range 0 1, lower values corresponding transparent colors.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/autoplot.spatial_initial_split.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a ggplot for a spatial initial rsplit. — autoplot.spatial_initial_split","text":"ggplot object fold assigned color, made using ggplot2::geom_sf().","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/autoplot.spatial_initial_split.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Create a ggplot for a spatial initial rsplit. — autoplot.spatial_initial_split","text":"plot method wrapper around standard spatial_rsplit method, relables folds Testing Traning following convention standard initial_split object","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/autoplot.spatial_initial_split.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a ggplot for a spatial initial rsplit. — autoplot.spatial_initial_split","text":"","code":"set.seed(123) block_initial <- spatial_initial_split(boston_canopy, prop = 1/5, spatial_block_cv) autoplot(block_initial)"},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/boyce_cont.html","id":null,"dir":"Reference","previous_headings":"","what":"Boyce continuous index (BCI) — boyce_cont","title":"Boyce continuous index (BCI) — boyce_cont","text":"function Boyce Continuous Index, measure model accuracy appropriate Species Distribution Models presence data (.e. using pseudoabsences background). algorithm used comes package enmSdm, uses multiple overlapping windows.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/boyce_cont.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Boyce continuous index (BCI) — boyce_cont","text":"","code":"boyce_cont(data, ...)  # S3 method for data.frame boyce_cont(   data,   truth,   ...,   estimator = NULL,   na_rm = TRUE,   event_level = \"first\",   case_weights = NULL )  # S3 method for sf boyce_cont(data, ...)  boyce_cont_vec(   truth,   estimate,   estimator = NULL,   na_rm = TRUE,   event_level = \"first\",   case_weights = NULL,   ... )"},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/boyce_cont.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Boyce continuous index (BCI) — boyce_cont","text":"data Either data.frame containing columns specified truth estimate arguments, table/matrix true class results columns table. ... set unquoted column names one dplyr selector functions choose variables contain class probabilities. truth binary, 1 column selected, correspond value event_level. Otherwise, many columns factor levels truth ordering columns factor levels truth. truth column identifier true class results (factor). unquoted column name although argument passed expression supports quasiquotation (can unquote column names). ⁠_vec()⁠ functions, factor vector. estimator One \"binary\", \"hand_till\", \"macro\", \"macro_weighted\" specify type averaging done. \"binary\" relevant two class case. others general methods calculating multiclass metrics. default automatically choose \"binary\" truth binary, \"hand_till\" truth >2 levels case_weights specified, \"macro\" truth >2 levels case_weights specified (case \"hand_till\" well-defined). na_rm logical value indicating whether NA values stripped computation proceeds. event_level single string. Either \"first\" \"second\" specify level truth consider \"event\". argument applicable estimator = \"binary\". default uses internal helper generally defaults \"first\" case_weights optional column identifier case weights. unquoted column name evaluates numeric column data. ⁠_vec()⁠ functions, numeric vector. estimate truth binary, numeric vector class probabilities corresponding \"relevant\" class. Otherwise, matrix many columns factor levels truth. assumed order levels truth.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/boyce_cont.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Boyce continuous index (BCI) — boyce_cont","text":"tibble columns .metric, .estimator, .estimate 1 row values. grouped data frames, number rows returned number groups.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/boyce_cont.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Boyce continuous index (BCI) — boyce_cont","text":"multiclass version function, operates binary predictions (e.g. presences absences SDMs).","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/boyce_cont.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Boyce continuous index (BCI) — boyce_cont","text":"Boyce, M.S., P.R. Vernier, S.E. Nielsen F.K.. Schmiegelow. 2002. Evaluating resource selection functions. Ecol. Model., 157, 281-300. Hirzel, .H., G. Le Lay, V. Helfer, C. Randin . Guisan. 2006. Evaluating ability habitat suitability models predict species presences. Ecol. Model., 199, 142-152.","code":""},{"path":[]},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/boyce_cont.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Boyce continuous index (BCI) — boyce_cont","text":"","code":"boyce_cont(two_class_example, truth, Class1) #> # A tibble: 1 × 3 #>   .metric    .estimator .estimate #>   <chr>      <chr>          <dbl> #> 1 boyce_cont binary         0.805"},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/calib_class_thresh.html","id":null,"dir":"Reference","previous_headings":"","what":"Calibrate class thresholds — calib_class_thresh","title":"Calibrate class thresholds — calib_class_thresh","text":"Predict new dataset using simple ensemble. Predictions individual models combined according fun","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/calib_class_thresh.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calibrate class thresholds — calib_class_thresh","text":"","code":"calib_class_thresh(object, class_thresh, metric_thresh = NULL)"},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/calib_class_thresh.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calibrate class thresholds — calib_class_thresh","text":"object simple_ensemble object class_thresh probability threshold used convert probabilities classes. can number (0 1), character metric (currently \"tss_max\" \"sensitivity\"). sensitivity, additional target value passed along second element vector, e.g. c(\"sensitivity\",0.8). metric_thresh vector length 2 giving metric threshold, used prune models ensemble used prediction. 'metrics' need computed workflow tuned. Examples c(\"accuracy\",0.8) c(\"boyce_cont\",0.7)","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/calib_class_thresh.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calibrate class thresholds — calib_class_thresh","text":"simple_ensemble object","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/calib_class_thresh.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calibrate class thresholds — calib_class_thresh","text":"","code":"test_ens <- simple_ensemble() %>% add_member(two_class_res[1:3, ], metric=\"roc_auc\") test_ens <- calib_class_thresh(test_ens, class_thresh=\"tss_max\") test_ens <- calib_class_thresh(test_ens, class_thresh=c(\"sens\",0.9))"},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/check_coords_names.html","id":null,"dir":"Reference","previous_headings":"","what":"Check that we have a valid pair of coordinate names — check_coords_names","title":"Check that we have a valid pair of coordinate names — check_coords_names","text":"internal function checks coords (passed functions) valid set names, , NULL, standard variable names data","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/check_coords_names.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Check that we have a valid pair of coordinate names — check_coords_names","text":"","code":"check_coords_names(data, coords)"},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/check_coords_names.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Check that we have a valid pair of coordinate names — check_coords_names","text":"data data.frame containing locations. coords vector length two giving names \"x\" \"y\" coordinates, points data.frame use standard names.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/check_coords_names.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Check that we have a valid pair of coordinate names — check_coords_names","text":"vector length 2 valid names, correct order","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/check_sdm_presence.html","id":null,"dir":"Reference","previous_headings":"","what":"Check that the column with presences is correctly formatted — check_sdm_presence","title":"Check that the column with presences is correctly formatted — check_sdm_presence","text":"tidysdm, string defining presences first level response factor. function checks column correctly formatted.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/check_sdm_presence.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Check that the column with presences is correctly formatted — check_sdm_presence","text":"","code":"check_sdm_presence(.data, .col, presence_level = \"presence\")"},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/check_sdm_presence.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Check that the column with presences is correctly formatted — check_sdm_presence","text":".data data.frame tibble, derived object sf data.frame .col column containing presences presence_level string used define presence level .col","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/check_sdm_presence.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Check that the column with presences is correctly formatted — check_sdm_presence","text":"TRUE correctly formatted","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/conf_matrix_df.html","id":null,"dir":"Reference","previous_headings":"","what":"Make a confusion matrix dataframe for multiple thresholds — conf_matrix_df","title":"Make a confusion matrix dataframe for multiple thresholds — conf_matrix_df","text":"Create confusion matrix multiple thresholds, using optimise tss","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/conf_matrix_df.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Make a confusion matrix dataframe for multiple thresholds — conf_matrix_df","text":"","code":"conf_matrix_df(presences, absences)"},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/conf_matrix_df.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Make a confusion matrix dataframe for multiple thresholds — conf_matrix_df","text":"presences Probabilities presences absences probabilities absences","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/conf_matrix_df.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Make a confusion matrix dataframe for multiple thresholds — conf_matrix_df","text":"data.frame thresholds columns thres, tp, fp, fn, tn","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/control_ensemble.html","id":null,"dir":"Reference","previous_headings":"","what":"Control wrappers — control_ensemble_grid","title":"Control wrappers — control_ensemble_grid","text":"Supply light wrappers control argument tune::tune_grid(), tune::tune_bayes(), tune::fit_resamples() call return needed elements use ensemble. functions return appropriate control grid ensure assessment set predictions information model specifications preprocessors, supplied resampling results object! integrate ensemble settings existing control settings, note functions just call appropriate tune::control_* function arguments save_pred = TRUE, save_workflow = TRUE. wrappers equivalent ones used stacks package.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/control_ensemble.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Control wrappers — control_ensemble_grid","text":"","code":"control_ensemble_grid()  control_ensemble_resamples()  control_ensemble_bayes()"},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/control_ensemble.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Control wrappers — control_ensemble_grid","text":"tune::control_grid, tune::control_bayes, tune::control_resamples object.","code":""},{"path":[]},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/dist_pres_vs_bg.html","id":null,"dir":"Reference","previous_headings":"","what":"Distance between the distribution of climate values for presences vs background — dist_pres_vs_bg","title":"Distance between the distribution of climate values for presences vs background — dist_pres_vs_bg","text":"environemntal variable, function computes density functions presences absences returns (1-overlap), measure distance two distributions. Variables high distance good candidates SDMs, species occurrences confined subset available background.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/dist_pres_vs_bg.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Distance between the distribution of climate values for presences vs background — dist_pres_vs_bg","text":"","code":"dist_pres_vs_bg(.data, .col)"},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/dist_pres_vs_bg.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Distance between the distribution of climate values for presences vs background — dist_pres_vs_bg","text":".data data.frame (derived object, tibble, sf) values bioclimate variables presences background .col column containing presences; assumes presences first level factor","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/dist_pres_vs_bg.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Distance between the distribution of climate values for presences vs background — dist_pres_vs_bg","text":"name vector distances","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/dist_pres_vs_bg.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Distance between the distribution of climate values for presences vs background — dist_pres_vs_bg","text":"","code":"# This should be updatd to use a dataset from tidysdm data(\"bradypus\", package=\"maxnet\") bradypus_tb <- tibble::as_tibble(bradypus) %>% dplyr::mutate(presence = relevel(factor(   dplyr::case_match (presence, 1~\"presence\",0 ~\"absence\")),   ref=\"presence\")) %>% select(-ecoreg)  bradypus_tb %>% dist_pres_vs_bg(presence) #> pre6190_l10 frs6190_ann tmn6190_ann pre6190_ann vap6190_ann  pre6190_l7  #>   0.4366687   0.4300214   0.4295160   0.4096046   0.3945916   0.3933181  #>       h_dem tmp6190_ann dtr6190_ann  pre6190_l4 tmx6190_ann cld6190_ann  #>   0.3647373   0.3317039   0.3288671   0.2544985   0.2417989   0.1811927  #>  pre6190_l1  #>   0.1296447"},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/filter_high_cor.html","id":null,"dir":"Reference","previous_headings":"","what":"Filter to retain only variables below a given correlation threshold — filter_high_cor","title":"Filter to retain only variables below a given correlation threshold — filter_high_cor","text":"function estimates correlation matrix among variables terra::SpatRaster returns vector variable names indeces certain cutoff. algorithm based caret::findCorrelation, using exact option. absolute values pair-wise correlations considered. two variables high correlation, function looks mean absolute correlation variable removes variable largest mean absolute correlation.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/filter_high_cor.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Filter to retain only variables below a given correlation threshold — filter_high_cor","text":"","code":"filter_high_cor(x, cutoff = 0.7, verbose = FALSE, names = TRUE)"},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/filter_high_cor.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Filter to retain only variables below a given correlation threshold — filter_high_cor","text":"x terra::SpatRaster object cutoff numeric value pair-wise absolute correlation cutoff verbose boolean printing details names logical; column names returned TRUE column index FALSE)?","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/filter_high_cor.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Filter to retain only variables below a given correlation threshold — filter_high_cor","text":"vector names columns correlation threhold (names = TRUE), otherwise vector indeces.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/filter_high_cor.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Filter to retain only variables below a given correlation threshold — filter_high_cor","text":"several function subselect can also used accomplish goal tend retain predictors.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/form_resp.html","id":null,"dir":"Reference","previous_headings":"","what":"Get the response variable from a formula — form_resp","title":"Get the response variable from a formula — form_resp","text":"counterpart rsample::form_pred.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/form_resp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get the response variable from a formula — form_resp","text":"","code":"form_resp(x)"},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/form_resp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get the response variable from a formula — form_resp","text":"x formula","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/form_resp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get the response variable from a formula — form_resp","text":"character name response","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/form_resp.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Get the response variable from a formula — form_resp","text":"Note: might behave well functions log(y). neither form_pred modified https://stackoverflow.com/questions/13217322/--reliably-get-dependent-variable-name--formula-object","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/gam_formula.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a formula for gam — gam_formula","title":"Create a formula for gam — gam_formula","text":"function takes formula recipe, turns numeric predictors smooths given k","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/gam_formula.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a formula for gam — gam_formula","text":"","code":"gam_formula(object, k = 10)"},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/gam_formula.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a formula for gam — gam_formula","text":"object recipes::recipe, already trained k k value smooth","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/gam_formula.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a formula for gam — gam_formula","text":"formula","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/geom_split_violin.html","id":null,"dir":"Reference","previous_headings":"","what":"Split violin geometry for ggplots — geom_split_violin","title":"Split violin geometry for ggplots — geom_split_violin","text":"geometry displays density distribution two groups side side, two halves violin. Note emptyx aesthetic provided even want plot single variable (see example ).","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/geom_split_violin.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Split violin geometry for ggplots — geom_split_violin","text":"","code":"geom_split_violin(   mapping = NULL,   data = NULL,   stat = \"ydensity\",   position = \"identity\",   nudge = 0,   ...,   draw_quantiles = NULL,   trim = TRUE,   scale = \"area\",   na.rm = FALSE,   show.legend = NA,   inherit.aes = TRUE )"},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/geom_split_violin.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Split violin geometry for ggplots — geom_split_violin","text":"mapping Set aesthetic mappings created aes(). specified inherit.aes = TRUE (default), combined default mapping top level plot. must supply mapping plot mapping. data data displayed layer. three options: NULL, default, data inherited plot data specified call ggplot(). data.frame, object, override plot data. objects fortified produce data frame. See fortify() variables created. function called single argument, plot data. return value must data.frame, used layer data. function can created formula (e.g. ~ head(.x, 10)). stat Use override default connection geom_violin() stat_ydensity(). position Position adjustment, either string naming adjustment (e.g. \"jitter\" use position_jitter), result call position adjustment function. Use latter need change settings adjustment. nudge Add space half-violin middle space allotted given factor x-axis. ... arguments passed layer(). often aesthetics, used set aesthetic fixed value, like colour = \"red\" size = 3. may also parameters paired geom/stat. draw_quantiles (NULL) (default), draw horizontal lines given quantiles density estimate. trim TRUE (default), trim tails violins range data. FALSE, trim tails. scale \"area\" (default), violins area (trimming tails). \"count\", areas scaled proportionally number observations. \"width\", violins maximum width. na.rm FALSE, default, missing values removed warning. TRUE, missing values silently removed. show.legend logical. layer included legends? NA, default, includes aesthetics mapped. FALSE never includes, TRUE always includes. can also named logical vector finely select aesthetics display. inherit.aes FALSE, overrides default aesthetics, rather combining . useful helper functions define data aesthetics inherit behaviour default plot specification, e.g. borders().","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/geom_split_violin.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Split violin geometry for ggplots — geom_split_violin","text":"ggplot2::layer object","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/geom_split_violin.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Split violin geometry for ggplots — geom_split_violin","text":"implementation based https://stackoverflow.com/questions/35717353/split-violin-plot--ggplot2. Credit goes @jan-jlx providing complete implementation StackOverflow, Trang Q. Nguyen adding nudge parameter.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/geom_split_violin.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Split violin geometry for ggplots — geom_split_violin","text":"","code":"data(\"bradypus\", package=\"maxnet\") bradypus_tb <- tibble::as_tibble(bradypus) %>% dplyr::mutate(presence = relevel(factor( dplyr::case_match (presence, 1~\"presence\",0 ~\"absence\")), ref=\"presence\"))  ggplot(bradypus_tb, aes(x = \"\",                         y= cld6190_ann,                         fill = presence)) +   geom_split_violin(nudge=0.01)"},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/grid_cellsize.html","id":null,"dir":"Reference","previous_headings":"","what":"Get default grid cellsize for a given dataset — grid_cellsize","title":"Get default grid cellsize for a given dataset — grid_cellsize","text":"function facilitates using spatialsample::spatial_block_cv multiple times analysis. spatialsample::spatial_block_cv creates grid based object data. However, spatial blocks generated multiple times analysis (e.g. spatial_initial_split(), subsequently cross-validation training dataset), might desirable keep grid). applying function largest dataset, usually full dataset spatial_initial_split(). resulting cellsize can used option spatialsample::spatial_block_cv.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/grid_cellsize.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get default grid cellsize for a given dataset — grid_cellsize","text":"","code":"grid_cellsize(data, n = c(10, 10))"},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/grid_cellsize.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get default grid cellsize for a given dataset — grid_cellsize","text":"data sf::sf dataset used size grid n number cells grid, defaults c(10,10), also default sf::st_make_grid()","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/grid_cellsize.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get default grid cellsize for a given dataset — grid_cellsize","text":"cell size","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/grid_offset.html","id":null,"dir":"Reference","previous_headings":"","what":"Get default grid cellsize for a given dataset — grid_offset","title":"Get default grid cellsize for a given dataset — grid_offset","text":"function facilitates using spatialsample::spatial_block_cv multiple times analysis. spatialsample::spatial_block_cv creates grid based object data. However, spatial blocks generated multiple times analysis (e.g. spatial_initial_split(), subsequently cross-validation training dataset), might desirable keep grid). applying function largest dataset, usually full dataset spatial_initial_split(). resulting cellsize can used option spatialsample::spatial_block_cv.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/grid_offset.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get default grid cellsize for a given dataset — grid_offset","text":"","code":"grid_offset(data)"},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/grid_offset.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get default grid cellsize for a given dataset — grid_offset","text":"data sf::sf dataset used size grid","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/grid_offset.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get default grid cellsize for a given dataset — grid_offset","text":"grid offset","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/horses.html","id":null,"dir":"Reference","previous_headings":"","what":"Coordinates of radiocarbon dates for horses — horses","title":"Coordinates of radiocarbon dates for horses — horses","text":"Coordinates presences horses 22k 8k YBP.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/horses.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Coordinates of radiocarbon dates for horses — horses","text":"","code":"horses"},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/horses.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Coordinates of radiocarbon dates for horses — horses","text":"tibble 1,297 rows 3 variables: latitude latitudes degrees longitude longitudes degrees time_bp time years present","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/km2m.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert a geographic distance from km to m — km2m","title":"Convert a geographic distance from km to m — km2m","text":"function takes distance km converts meters, units generally used geographic operations R. trivial conversion, functions ensures zeroes lost along way!","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/km2m.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert a geographic distance from km to m — km2m","text":"","code":"km2m(x)"},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/km2m.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert a geographic distance from km to m — km2m","text":"x number km","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/km2m.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Convert a geographic distance from km to m — km2m","text":"number meters","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/km2m.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Convert a geographic distance from km to m — km2m","text":"","code":"km2m(10000) #> [1] 1e+07 km2m(1) #> [1] 1000"},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/lacerta.html","id":null,"dir":"Reference","previous_headings":"","what":"Coordinates of presences for Iberian emerald lizard — lacerta","title":"Coordinates of presences for Iberian emerald lizard — lacerta","text":"Coordinates presences Lacerta schreiberi. variables follows:","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/lacerta.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Coordinates of presences for Iberian emerald lizard — lacerta","text":"","code":"lacerta"},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/lacerta.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Coordinates of presences for Iberian emerald lizard — lacerta","text":"tibble 1,297 rows 3 variables: ID ids GBIF latitude latitudes degrees longitude longitudes degrees","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/maxent.html","id":null,"dir":"Reference","previous_headings":"","what":"Maxent model — maxent","title":"Maxent model — maxent","text":"maxent defines MaxEnt model binary outcomes used Species Distribution Models. good guide options Maxent model work can found https://onlinelibrary.wiley.com/doi/full/10.1111/j.1600-0587.2013.07872.x","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/maxent.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Maxent model — maxent","text":"","code":"maxent(   mode = \"classification\",   engine = \"maxnet\",   feature_classes = NULL,   regularization_multiplier = NULL )"},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/maxent.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Maxent model — maxent","text":"mode single character string type model. possible value model \"classification\". engine single character string specifying computational engine use fitting. Currently \"maxnet\" available. feature_classes character, continuous feature classes desired, either \"default\" subset \"lqpht\" (example, \"lh\") regularization_multiplier numeric, constant adjust regularization","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/maxent.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Maxent model — maxent","text":"model_spec maxent model","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/maxent.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Maxent model — maxent","text":"","code":"# format the data data(\"bradypus\", package=\"maxnet\") bradypus_tb <- tibble::as_tibble(bradypus) %>% dplyr::mutate(presence = relevel(factor(   dplyr::case_match (presence, 1~\"presence\",0 ~\"absence\")),   ref=\"presence\")) %>% select(-ecoreg)  # fit the model, and make some predictions maxent_spec <- maxent(feature_classes = \"lq\") maxent_fitted <- maxent_spec %>%   fit(presence ~ ., data = bradypus_tb) pred_prob <-predict(maxent_fitted,new_data = bradypus[,-1], type=\"prob\") pred_class <- predict(maxent_fitted,new_data = bradypus[,-1], type=\"class\")  # Now with tuning maxent_spec <- maxent(regularization_multiplier = tune(),                       feature_classes = tune()) set.seed(452) cv <- vfold_cv(bradypus_tb, v=2) maxent_tune_res <- maxent_spec %>%   tune_grid(presence ~ ., cv, grid = 3) show_best(maxent_tune_res, metric = \"roc_auc\") #> # A tibble: 3 × 8 #>   feature_classes regularization_multip…¹ .metric .estimator  mean     n std_err #>   <chr>                             <dbl> <chr>   <chr>      <dbl> <int>   <dbl> #> 1 l                                  1.02 roc_auc binary     0.857     2  0.0143 #> 2 lqph                               1.90 roc_auc binary     0.856     2  0.0121 #> 3 lqph                               2.50 roc_auc binary     0.854     2  0.0123 #> # ℹ abbreviated name: ¹​regularization_multiplier #> # ℹ 1 more variable: .config <chr>"},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/maxent_params.html","id":null,"dir":"Reference","previous_headings":"","what":"Parameters for maxent models — maxent_params","title":"Parameters for maxent models — maxent_params","text":"parameters auxiliary MaxEnt models using \"maxnet\" engine. functions used tuning functions, user rarely access directly.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/maxent_params.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Parameters for maxent models — maxent_params","text":"","code":"regularization_multiplier(range = c(0.5, 3), trans = NULL)  feature_classes(values = c(\"l\", \"lq\", \"lqp\", \"lqph\", \"lqpht\"))"},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/maxent_params.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Parameters for maxent models — maxent_params","text":"range two-element vector holding defaults smallest largest possible values, respectively. transformation specified, values transformed units. trans trans object scales package, scales::log10_trans() scales::reciprocal_trans(). provided, default used matches units used range. transformation, NULL. values feature_classes(), character string subset \"lqpht\" (example, \"lh\")","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/maxent_params.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Parameters for maxent models — maxent_params","text":"param object can used tuning.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/maxent_params.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Parameters for maxent models — maxent_params","text":"","code":"regularization_multiplier() #> Reg. multiplier (quantitative) #> Range: [0.5, 3] feature_classes() #> Feature classes  (qualitative) #> 5 possible values include: #> 'l', 'lq', 'lqp', 'lqph' and 'lqpht'"},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/maxnet_fit.html","id":null,"dir":"Reference","previous_headings":"","what":"Wrapper to fit maxnet models with formulae — maxnet_fit","title":"Wrapper to fit maxnet models with formulae — maxnet_fit","text":"function wrapper around maxnet::maxnet, takes formula data well exposing parameters normalisation manner compatible parsnip. Users unlikely use function directly.  parsnip model specification MaxEnt, see maxent().","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/maxnet_fit.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Wrapper to fit maxnet models with formulae — maxnet_fit","text":"","code":"maxnet_fit(   formula,   data,   regmult = 1,   classes = \"default\",   regfun = maxnet::maxnet.default.regularization,   addsamplestobackground = T,   ... )"},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/maxnet_fit.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Wrapper to fit maxnet models with formulae — maxnet_fit","text":"formula formula defining outcome predictors data data.frame outcomes predictors regmult numeric, constant adjust regularization classes character, continuous feature classes desired, either \"default\" subset \"lqpht\" (example, \"lh\") regfun function, computes regularization constant feature addsamplestobackground logical, TRUE add background presence sample already ... currently used.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/maxnet_fit.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Wrapper to fit maxnet models with formulae — maxnet_fit","text":"Maxnet returns object class maxnet, list consisting glmnet model following elements added: betas nonzero coefficients fitted model alpha constant offset making exponential model sum one background data entropy entropy exponential model penalty.factor regularization constants used feature featuremins minimum feature, used clamping featuremaxs maximum feature, used clamping varmin minimum predictor, used clamping varmax maximum predictor, used clamping samplemeans mean predictor samples (majority factors) levels levels predictor factor","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/maxnet_fit.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Wrapper to fit maxnet models with formulae — maxnet_fit","text":"response needs factor class representing presences reference level factor (expected classification models). good guide options Maxent model work can found https://onlinelibrary.wiley.com/doi/full/10.1111/j.1600-0587.2013.07872.x","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/maxnet_fit.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Wrapper to fit maxnet models with formulae — maxnet_fit","text":"","code":"if (FALSE) {   # we repeat the example in the `maxnet` package   data(\"bradypus\", package=\"maxnet\")   bradypus_tb <- tibble::as_tibble(bradypus) %>% dplyr::mutate(presence = relevel(factor(     dplyr::case_match (presence, 1~\"presence\",0 ~\"absence\")),     ref=\"presence\"))   mod3 <- maxnet_fit(presence~.,data=bradypus_tb, classes=\"lq\")   plot(mod, \"tmp6190_ann\") }"},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/maxnet_predict.html","id":null,"dir":"Reference","previous_headings":"","what":"Wrapper to predict maxnet models — maxnet_predict","title":"Wrapper to predict maxnet models — maxnet_predict","text":"function wrapper around predict method maxnet::maxnet, making function compatible parsnip. Users unlikely use function directly.  parsnip model specification MaxEnt, see maxent().","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/maxnet_predict.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Wrapper to predict maxnet models — maxnet_predict","text":"","code":"maxnet_predict(   object,   newdata,   type = c(\"class\", \"prob\"),   maxnet_type = c(\"cloglog\", \"link\", \"exponential\", \"logistic\"),   clamp = TRUE )"},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/maxnet_predict.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Wrapper to predict maxnet models — maxnet_predict","text":"object maxnet::maxnet object newdata dataframe new data type either \"prob\" \"class\" maxnet_type transformation used prediction clamp logical, definign whether clamping observed ranges used","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/maxnet_predict.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Wrapper to predict maxnet models — maxnet_predict","text":"tibble predictions","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/optim_thresh.html","id":null,"dir":"Reference","previous_headings":"","what":"Find threshold that optimises a given metric — optim_thresh","title":"Find threshold that optimises a given metric — optim_thresh","text":"function returns threshold turn probabilities binary classes whilst optimising given metric. Currently available tss_max sensitivity (target sensitivity required).","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/optim_thresh.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Find threshold that optimises a given metric — optim_thresh","text":"","code":"optim_thresh(truth, estimate, metric, event_level = \"first\")"},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/optim_thresh.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Find threshold that optimises a given metric — optim_thresh","text":"truth column identifier true class results (factor). unquoted column name although argument passed expression supports quasiquotation (can unquote column names). ⁠_vec()⁠ functions, factor vector. estimate predicted probability event metric character metric optimised. Currently \"tss_max\", \"sensitivity\" given target (e.g. c(\"sensitivity\",0.8)) event_level single string. Either \"first\" \"second\" specify level truth consider \"event\". argument applicable estimator = \"binary\". default uses internal helper generally defaults \"first\"","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/optim_thresh.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Find threshold that optimises a given metric — optim_thresh","text":"probability threshold event","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/optim_thresh.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Find threshold that optimises a given metric — optim_thresh","text":"","code":"optim_thresh(two_class_example$truth,two_class_example$Class1,metric=c(\"tss_max\")) #> [1] 0.7544818 optim_thresh(two_class_example$truth,two_class_example$Class1,metric=c(\"sens\",0.9)) #> [1] 0.3710924"},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/optim_thresh_sens.html","id":null,"dir":"Reference","previous_headings":"","what":"Find threshold that gives a target sensitivity — optim_thresh_sens","title":"Find threshold that gives a target sensitivity — optim_thresh_sens","text":"internal function returns threshold turn probabilities binary classes given target sensitivity","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/optim_thresh_sens.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Find threshold that gives a target sensitivity — optim_thresh_sens","text":"","code":"optim_thresh_sens(presences, absences, sens_target)"},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/optim_thresh_sens.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Find threshold that gives a target sensitivity — optim_thresh_sens","text":"presences Probabilities presences. absences Provabilities absences sens_target target sensitivity","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/optim_thresh_sens.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Find threshold that gives a target sensitivity — optim_thresh_sens","text":"probability threshold event","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/optim_thresh_tss_max.html","id":null,"dir":"Reference","previous_headings":"","what":"Find threshold that maximises TSS — optim_thresh_tss_max","title":"Find threshold that maximises TSS — optim_thresh_tss_max","text":"internal function returns threshold turn probabilities binary classes maximise TSS","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/optim_thresh_tss_max.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Find threshold that maximises TSS — optim_thresh_tss_max","text":"","code":"optim_thresh_tss_max(presences, absences)"},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/optim_thresh_tss_max.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Find threshold that maximises TSS — optim_thresh_tss_max","text":"presences Probabilities presences. absences Provabilities absences","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/optim_thresh_tss_max.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Find threshold that maximises TSS — optim_thresh_tss_max","text":"probability threshold event","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/pipe.html","id":null,"dir":"Reference","previous_headings":"","what":"Pipe operator — %>%","title":"Pipe operator — %>%","text":"See magrittr::%>% details.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/pipe.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Pipe operator — %>%","text":"","code":"lhs %>% rhs"},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/pipe.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Pipe operator — %>%","text":"lhs value magrittr placeholder. rhs function call using magrittr semantics.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/pipe.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Pipe operator — %>%","text":"result calling rhs(lhs).","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/plot_pres_vs_bg.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot presences vs background — plot_pres_vs_bg","title":"Plot presences vs background — plot_pres_vs_bg","text":"Create composite plots contrasting distribution multiple variables presences vs background.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/plot_pres_vs_bg.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot presences vs background — plot_pres_vs_bg","text":"","code":"plot_pres_vs_bg(.data, .col)"},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/plot_pres_vs_bg.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot presences vs background — plot_pres_vs_bg","text":".data data.frame (derived object, tibble, sf) values bioclimate variables presences background .col column containing presences; assumes presences first level factor","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/plot_pres_vs_bg.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot presences vs background — plot_pres_vs_bg","text":"patchwork composite plot","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/plot_pres_vs_bg.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plot presences vs background — plot_pres_vs_bg","text":"","code":"data(\"bradypus\", package=\"maxnet\") bradypus_tb <- tibble::as_tibble(bradypus) %>% dplyr::mutate(presence = relevel(factor(   dplyr::case_match (presence, 1~\"presence\",0 ~\"absence\")),   ref=\"presence\")) %>% select(-ecoreg)  bradypus_tb %>% plot_pres_vs_bg(presence)"},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/predict.repeat_ensemble.html","id":null,"dir":"Reference","previous_headings":"","what":"Predict for a simple ensemble set — predict.repeat_ensemble","title":"Predict for a simple ensemble set — predict.repeat_ensemble","text":"Predict new dataset using simple ensemble. Predictions individual models combined according fun","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/predict.repeat_ensemble.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Predict for a simple ensemble set — predict.repeat_ensemble","text":"","code":"# S3 method for repeat_ensemble predict(   object,   new_data,   type = \"prob\",   fun = \"mean\",   metric_thresh = NULL,   class_thresh = NULL,   members = FALSE,   ... )"},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/predict.repeat_ensemble.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Predict for a simple ensemble set — predict.repeat_ensemble","text":"object simple_ensemble object new_data data fit (usually full training dataset) type type prediction, \"prob\" \"class\". fun string defining aggregating function. can take values mean, median, weighted_mean, weighted_median none. possible combine multiple functions, except \"none\". set \"none\", individual member predictions returned (automatically sets member TRUE) metric_thresh vector length 2 giving metric threshold, used prune models ensemble used prediction. 'metrics' need computed workflow tuned. Examples c(\"accuracy\",0.8) c(\"boyce_cont\",0.7) class_thresh probability threshold used convert probabilities classes. can number (0 1), character metric (currently \"tss_max\" \"sensitivity\"). sensitivity, additional target value passed along second element vector, e.g. c(\"sensitivity\",0.8). members boolean defining whether individual predictions member added ensemble prediction. columns individual members name workflow prefix, separated \".\" usual column names predictions. ... used method.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/predict.repeat_ensemble.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Predict for a simple ensemble set — predict.repeat_ensemble","text":"tibble predictions","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/predict.simple_ensemble.html","id":null,"dir":"Reference","previous_headings":"","what":"Predict for a simple ensemble set — predict.simple_ensemble","title":"Predict for a simple ensemble set — predict.simple_ensemble","text":"Predict new dataset using simple ensemble. Predictions individual models combined according fun","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/predict.simple_ensemble.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Predict for a simple ensemble set — predict.simple_ensemble","text":"","code":"# S3 method for simple_ensemble predict(   object,   new_data,   type = \"prob\",   fun = \"mean\",   metric_thresh = NULL,   class_thresh = NULL,   members = FALSE,   ... )"},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/predict.simple_ensemble.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Predict for a simple ensemble set — predict.simple_ensemble","text":"object simple_ensemble object new_data data fit (usually full training dataset) type type prediction, \"prob\" \"class\". fun string defining aggregating function. can take values mean, median, weighted_mean, weighted_median none. possible combine multiple functions, except \"none\". set \"none\", individual member predictions returned (automatically sets member TRUE) metric_thresh vector length 2 giving metric threshold, used prune models ensemble used prediction. 'metrics' need computed workflow tuned. Examples c(\"accuracy\",0.8) c(\"boyce_cont\",0.7) class_thresh probability threshold used convert probabilities classes. can number (0 1), character metric (currently \"tss_max\" \"sensitivity\"). sensitivity, additional target value passed along second element vector, e.g. c(\"sensitivity\",0.8). members boolean defining whether individual predictions member added ensemble prediction. columns individual members name workflow prefix, separated \".\" usual column names predictions. ... used method.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/predict.simple_ensemble.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Predict for a simple ensemble set — predict.simple_ensemble","text":"tibble predictions","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/predict_raster.html","id":null,"dir":"Reference","previous_headings":"","what":"Make predictions for a whole raster — predict_raster","title":"Make predictions for a whole raster — predict_raster","text":"function allows use raster data make predictions variety tidymodels objects, simple_ensemble stacks::linear_stack","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/predict_raster.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Make predictions for a whole raster — predict_raster","text":"","code":"predict_raster(object, raster, ...)  # S3 method for default predict_raster(object, raster, ...)"},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/predict_raster.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Make predictions for a whole raster — predict_raster","text":"object tidymodels object interest raster terra::SpatRaster input data. include levels names variables used object ... parameters passed standard predict() function appropriate object type.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/predict_raster.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Make predictions for a whole raster — predict_raster","text":"terra::SpatRaster predictions","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/prob_metrics.html","id":null,"dir":"Reference","previous_headings":"","what":"Probability metrics for sf objects — prob_metrics","title":"Probability metrics for sf objects — prob_metrics","text":"tidysdm provides methods handle sf::sf objects following yardstick metrics: yardstick::average_precision() yardstick::brier_class() yardstick::classification_cost() yardstick::gain_capture() yardstick::mn_log_loss() yardstick::pr_auc() yardstick::roc_auc() yardstick::roc_aunp() yardstick::roc_aunu()","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/prob_metrics.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Probability metrics for sf objects — prob_metrics","text":"","code":"# S3 method for sf average_precision(data, ...)  # S3 method for sf brier_class(data, ...)  # S3 method for sf classification_cost(data, ...)  # S3 method for sf gain_capture(data, ...)  # S3 method for sf mn_log_loss(data, ...)  # S3 method for sf pr_auc(data, ...)  # S3 method for sf roc_auc(data, ...)  # S3 method for sf roc_aunp(data, ...)  # S3 method for sf roc_aunu(data, ...)"},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/prob_metrics.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Probability metrics for sf objects — prob_metrics","text":"data sf::sf object ... parameters pass data.frame version metric.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/prob_metrics.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Probability metrics for sf objects — prob_metrics","text":"tibble columns .metric, .estimator, .estimate 1 row values.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/prob_to_binary.html","id":null,"dir":"Reference","previous_headings":"","what":"simple function to convert probability to binary classes — prob_to_binary","title":"simple function to convert probability to binary classes — prob_to_binary","text":"simple function convert probability binary classes","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/prob_to_binary.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"simple function to convert probability to binary classes — prob_to_binary","text":"","code":"prob_to_binary(x, thresh, class_levels)"},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/prob_to_binary.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"simple function to convert probability to binary classes — prob_to_binary","text":"x vector probabilities thresh threshold convert binary class_levels binary levels","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/prob_to_binary.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"simple function to convert probability to binary classes — prob_to_binary","text":"vector binary values","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/recipe.sf.html","id":null,"dir":"Reference","previous_headings":"","what":"Recipe for sf objects — recipe.sf","title":"Recipe for sf objects — recipe.sf","text":"method recipes::recipe() handles case x sf::sf object, commonly used Species Distribution Model.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/recipe.sf.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Recipe for sf objects — recipe.sf","text":"","code":"# S3 method for sf recipe(x, ...)"},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/recipe.sf.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Recipe for sf objects — recipe.sf","text":"x sf::sf data frame. ... parameters passed recipes::recipe()","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/recipe.sf.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Recipe for sf objects — recipe.sf","text":"object class recipes::recipe , see manpage recipes::recipe() details.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/recipe.sf.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Recipe for sf objects — recipe.sf","text":"recipes natively compatible sf::sf objects. problem geometry column sf::sf objects list, incompatible translation formulae recipe. method strips geometry column data.frame operations, thus allowing usual processing recipe() succeed. NOTE order matters! need use syntax recipe(x=sf_obj, formula=class~.) method successfully detect sf::sf object. Starting formula fail.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/repeat_ensemble.html","id":null,"dir":"Reference","previous_headings":"","what":"Repeat ensemble — repeat_ensemble","title":"Repeat ensemble — repeat_ensemble","text":"ensemble based multiple sets pseudoabsences/background. object collection (list) simple_ensemble objects predictions combined simple way (e.g. taking either mean median). simple_ensemble contains best version given model type following turning; simple ensembles need metric estimated cv process.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/repeat_ensemble.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Repeat ensemble — repeat_ensemble","text":"","code":"repeat_ensemble(...)"},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/repeat_ensemble.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Repeat ensemble — repeat_ensemble","text":"... used, function just creates empty repeat_ensemble object. Members added add_best_candidates()","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/repeat_ensemble.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Repeat ensemble — repeat_ensemble","text":"empty repeat_ensemble","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/sample_pseudoabs.html","id":null,"dir":"Reference","previous_headings":"","what":"Sample pseudo-absence (or background) points for SDM analysis — sample_pseudoabs","title":"Sample pseudo-absence (or background) points for SDM analysis — sample_pseudoabs","text":"function samples pseudo-absence (background, naming matter semantics) points raster given set presences. locations returned center points sampled cells, can overlap presences. following methods implemented: /itemize: /item: 'random': pseudo-absences/background randomly sampled region covered raster (.e. NAs). /item: 'dist_min': pseudo-absences/background randomly sampled region excluding buffer 'dist_min' presences (distances 'm' latlong rasters, map units projected rasters). /item: 'dist_max': pseudo-absences/background randomly sampled unioned buffers 'dist_max' presences (distances 'm' latlong rasters, map units projected rasters). Using union buffers means areas multiple buffers oversampled. also referred \"thickening\". /item: 'dist_disc': pseudo-absences/background randomly sampled unioned discs around presences two values 'dist_disc' defining minimum maximum distance presences.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/sample_pseudoabs.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Sample pseudo-absence (or background) points for SDM analysis — sample_pseudoabs","text":"","code":"sample_pseudoabs(   data,   raster,   n,   coords = NULL,   method = \"random\",   class_label = \"pseudoabs\",   return_pres = TRUE )"},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/sample_pseudoabs.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Sample pseudo-absence (or background) points for SDM analysis — sample_pseudoabs","text":"data sf::sf data frame, data frame coordinate variables. can defined coords, unless standard names (see details ). raster terra::SpatRaster cells sampled n number pseudoabsence/background points sample coords vector length two giving names \"x\" \"y\" coordinates, found data. left NULL, function try guess columns based standard names c(\"x\", \"y\"), c(\"X\",\"Y\"), c(\"longitude\", \"latitude\"), c(\"lon\", \"lat\") method sampling method. One 'random', 'dist_min', 'dist_max', 'dist_disc'. Threshold distances set additional elements vector, e.g c('dist_min',70000) c('dist_disc',50000,200000). class_label label given sampled points. Defaults pseudoabs return_pres return presences together pseudoabsences/background single tibble","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/sample_pseudoabs.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Sample pseudo-absence (or background) points for SDM analysis — sample_pseudoabs","text":"object class tibble::tibble. presences returned, presence level set reference (match expectations yardstick package considers first level event)","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/sample_pseudoabs_time.html","id":null,"dir":"Reference","previous_headings":"","what":"Sample pseudo-absence (or background) points for SDM analysis — sample_pseudoabs_time","title":"Sample pseudo-absence (or background) points for SDM analysis — sample_pseudoabs_time","text":"function samples pseudo-absence (background, naming matter semantics) points raster given set presences. locations returned center points sampled cells, can overlap presences. following methods implemented: /itemize: /item: 'random': pseudo-absences/background randomly sampled region covered raster (.e. NAs). /item: 'dist_min': pseudo-absences/background randomly sampled region excluding buffer 'dist_min' presences (distances 'm' latlong rasters, map units projected rasters). /item: 'dist_max': pseudo-absences/background randomly sampled unioned buffers 'dist_max' presences (distances 'm' latlong rasters, map units projected rasters). Using union buffers means areas multiple buffers oversampled. also referred \"thickening\". /item: 'dist_disc': pseudo-absences/background randomly sampled unioned discs around presences two values 'dist_disc' defining minimum maximum distance presences.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/sample_pseudoabs_time.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Sample pseudo-absence (or background) points for SDM analysis — sample_pseudoabs_time","text":"","code":"sample_pseudoabs_time(   data,   raster,   n_per_presence,   coords = NULL,   time_col = \"time\",   lubridate_fun = c,   method = \"random\",   class_label = \"pseudoabs\",   return_pres = TRUE )"},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/sample_pseudoabs_time.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Sample pseudo-absence (or background) points for SDM analysis — sample_pseudoabs_time","text":"data sf::sf data frame, data frame coordinate variables. can defined coords, unless standard names (see details ). raster terra::SpatRaster cells sampled n_per_presence number pseudoabsence/background points sample presence coords vector length two giving names \"x\" \"y\" coordinates, found data. left NULL, function try guess columns based standard names c(\"x\", \"y\"), c(\"X\",\"Y\"), c(\"longitude\", \"latitude\"), c(\"lon\", \"lat\") time_col name column time; time lubridate object, use lubridate_fun provide function can used convert appropriately lubridate_fun function convert time column lubridate object method sampling method. One 'random', 'dist_min', 'dist_max', 'dist_disc'. class_label label given sampled points. Defaults pseudoabs return_pres return presences together pseudoabsences/background single tibble","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/sample_pseudoabs_time.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Sample pseudo-absence (or background) points for SDM analysis — sample_pseudoabs_time","text":"object class tibble::tibble. presences returned, presence level set reference (match expectations yardstick package considers first level event)","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/sdm_metric_set.html","id":null,"dir":"Reference","previous_headings":"","what":"Metric set for SDM — sdm_metric_set","title":"Metric set for SDM — sdm_metric_set","text":"function returns yardstick::metric_set includes boyce_cont(), yardstick::roc_auc() tss_max(), commonly used metrics SDM.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/sdm_metric_set.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Metric set for SDM — sdm_metric_set","text":"","code":"sdm_metric_set(...)"},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/sdm_metric_set.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Metric set for SDM — sdm_metric_set","text":"... additional metrics added yardstick::metric_set. See help yardstick::metric_set() constraints type metrics can mixed.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/sdm_metric_set.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Metric set for SDM — sdm_metric_set","text":"yardstick::metric_set object.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/sdm_metric_set.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Metric set for SDM — sdm_metric_set","text":"","code":"sdm_metric_set() #> # A tibble: 3 × 3 #>   metric     class       direction #>   <chr>      <chr>       <chr>     #> 1 roc_auc    prob_metric maximize  #> 2 boyce_cont prob_metric maximize  #> 3 tss_max    prob_metric maximize  sdm_metric_set(accuracy) #> # A tibble: 4 × 3 #>   metric     class        direction #>   <chr>      <chr>        <chr>     #> 1 roc_auc    prob_metric  maximize  #> 2 boyce_cont prob_metric  maximize  #> 3 tss_max    prob_metric  maximize  #> 4 accuracy   class_metric maximize"},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/sdm_spec_boost_tree.html","id":null,"dir":"Reference","previous_headings":"","what":"Model specification for a Boosted Trees model for SDM — sdm_spec_boost_tree","title":"Model specification for a Boosted Trees model for SDM — sdm_spec_boost_tree","text":"function returns parsnip::model_spec Boosted Trees model used classifier presences absences Species Distribution Model.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/sdm_spec_boost_tree.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Model specification for a Boosted Trees model for SDM — sdm_spec_boost_tree","text":"","code":"sdm_spec_boost_tree(..., tune = c(\"sdm\", \"all\", \"custom\", \"none\"))"},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/sdm_spec_boost_tree.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Model specification for a Boosted Trees model for SDM — sdm_spec_boost_tree","text":"... parameters passed parsnip::boost_tree() customise model. See help function details. tune character defining tuning strategy. Valid strategies : itemize: /item: \"sdm\" chooses hyperparameters important tune sdm (boost_tree: 'mtry', 'trees', 'tree_depth', 'learn_rate', 'loss_reduction', 'stop_iter') /item: \"\" tunes hyperparameters (boost_tree: 'mtry', 'trees', 'tree_depth', 'learn_rate', 'loss_reduction', 'stop_iter','min_n' 'sample_size') /item: \"custom\" passes options '...' /item: \"none\" tune hyperparameter","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/sdm_spec_boost_tree.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Model specification for a Boosted Trees model for SDM — sdm_spec_boost_tree","text":"parsnip::model_spec model.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/sdm_spec_boost_tree.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Model specification for a Boosted Trees model for SDM — sdm_spec_boost_tree","text":"","code":"standard_bt_spec <- sdm_spec_boost_tree() full_bt_spec <- sdm_spec_boost_tree(tune = \"all\") custom_bt_spec <- sdm_spec_boost_tree(tune = \"custom\", mtry = tune())"},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/sdm_spec_gam.html","id":null,"dir":"Reference","previous_headings":"","what":"Model specification for a GAM for SDM — sdm_spec_gam","title":"Model specification for a GAM for SDM — sdm_spec_gam","text":"function returns parsnip::model_spec General Additive Model used classifier presences absences Species Distribution Model.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/sdm_spec_gam.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Model specification for a GAM for SDM — sdm_spec_gam","text":"","code":"sdm_spec_gam(..., tune = \"none\")"},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/sdm_spec_gam.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Model specification for a GAM for SDM — sdm_spec_gam","text":"... parameters passed parsnip::gen_additive_mod() customise model. See help function details. tune character defining tuning strategy. hyperparameters tune gam, valid option \"none\". parameter present consistency sdm_spec_* functions, nothing case.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/sdm_spec_gam.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Model specification for a GAM for SDM — sdm_spec_gam","text":"parsnip::model_spec model.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/sdm_spec_gam.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Model specification for a GAM for SDM — sdm_spec_gam","text":"","code":"my_gam_spec <- sdm_spec_gam()"},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/sdm_spec_glm.html","id":null,"dir":"Reference","previous_headings":"","what":"Model specification for a GLM for SDM — sdm_spec_glm","title":"Model specification for a GLM for SDM — sdm_spec_glm","text":"function returns parsnip::model_spec Generalised Linear Model used classifier presences absences Species Distribution Model.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/sdm_spec_glm.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Model specification for a GLM for SDM — sdm_spec_glm","text":"","code":"sdm_spec_glm(..., tune = \"none\")"},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/sdm_spec_glm.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Model specification for a GLM for SDM — sdm_spec_glm","text":"... parameters passed parsnip::logistic_reg() customise model. See help function details. tune character defining tuning strategy. hyperparameters tune glm, valid option \"none\". parameter present consistency sdm_spec_* functions, nothing case.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/sdm_spec_glm.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Model specification for a GLM for SDM — sdm_spec_glm","text":"parsnip::model_spec model.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/sdm_spec_glm.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Model specification for a GLM for SDM — sdm_spec_glm","text":"","code":"my_spec_glm <- sdm_spec_glm()"},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/sdm_spec_maxent.html","id":null,"dir":"Reference","previous_headings":"","what":"Model specification for a MaxEnt for SDM — sdm_spec_maxent","title":"Model specification for a MaxEnt for SDM — sdm_spec_maxent","text":"function returns parsnip::model_spec MaxEnt model used classifier presences absences Species Distribution Models.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/sdm_spec_maxent.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Model specification for a MaxEnt for SDM — sdm_spec_maxent","text":"","code":"sdm_spec_maxent(..., tune = c(\"sdm\", \"all\", \"custom\", \"none\"))"},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/sdm_spec_maxent.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Model specification for a MaxEnt for SDM — sdm_spec_maxent","text":"... parameters passed maxent() customise model. See help function details. tune character defining tuning strategy. Valid strategies : itemize: /item: \"sdm\" chooses hyperparameters important tune sdm (maxent, 'mtry') /item: \"\" tunes hyperparameters (maxent, 'mtry', 'trees' 'min') /item: \"custom\" passes options '...' /item: \"none\" tune hyperparameter","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/sdm_spec_maxent.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Model specification for a MaxEnt for SDM — sdm_spec_maxent","text":"parsnip::model_spec model.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/sdm_spec_maxent.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Model specification for a MaxEnt for SDM — sdm_spec_maxent","text":"","code":"test_maxent_spec <- sdm_spec_maxent(tune=\"sdm\") test_maxent_spec #> maxent Model Specification (classification) #>  #> Main Arguments: #>   feature_classes = tune() #>   regularization_multiplier = tune() #>  #> Computational engine: maxnet  #>  # setting specific values sdm_spec_maxent(tune=\"custom\", feature_classes=\"lq\") #> maxent Model Specification (classification) #>  #> Main Arguments: #>   feature_classes = lq #>  #> Computational engine: maxnet  #>"},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/sdm_spec_rand_forest.html","id":null,"dir":"Reference","previous_headings":"","what":"Model specification for a Random Forest for SDM — sdm_spec_rand_forest","title":"Model specification for a Random Forest for SDM — sdm_spec_rand_forest","text":"function returns parsnip::model_spec Random Forest used classifier presences absences Species Distribution Models.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/sdm_spec_rand_forest.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Model specification for a Random Forest for SDM — sdm_spec_rand_forest","text":"","code":"sdm_spec_rand_forest(..., tune = c(\"sdm\", \"all\", \"custom\", \"none\"))  sdm_spec_rf(..., tune = c(\"sdm\", \"all\", \"custom\", \"none\"))"},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/sdm_spec_rand_forest.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Model specification for a Random Forest for SDM — sdm_spec_rand_forest","text":"... parameters passed parsnip::rand_forest() customise model. See help function details. tune character defining tuning strategy. Valid strategies : itemize: /item: \"sdm\" chooses hyperparameters important tune sdm (rf, 'mtry') /item: \"\" tunes hyperparameters (rf, 'mtry', 'trees' 'min') /item: \"custom\" passes options '...' /item: \"none\" tune hyperparameter","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/sdm_spec_rand_forest.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Model specification for a Random Forest for SDM — sdm_spec_rand_forest","text":"parsnip::model_spec model.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/sdm_spec_rand_forest.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Model specification for a Random Forest for SDM — sdm_spec_rand_forest","text":"sdm_spec_rf() simply short form sm_spec_rand_forest().","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/sdm_spec_rand_forest.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Model specification for a Random Forest for SDM — sdm_spec_rand_forest","text":"","code":"test_rf_spec <- sdm_spec_rf(tune=\"sdm\") test_rf_spec #> Random Forest Model Specification (classification) #>  #> Main Arguments: #>   mtry = tune() #>  #> Computational engine: ranger  #>  # combining tuning with specific values for other hyperparameters sdm_spec_rf(tune=\"sdm\", trees=100) #> Random Forest Model Specification (classification) #>  #> Main Arguments: #>   mtry = tune() #>   trees = 100 #>  #> Computational engine: ranger  #>"},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/simple_ensemble.html","id":null,"dir":"Reference","previous_headings":"","what":"Simple ensemble — simple_ensemble","title":"Simple ensemble — simple_ensemble","text":"simple ensemble collection workflows predictions combined simple way (e.g. taking either mean median). Usually workflows consists best version given model type following turning. workflows fitted full training dataset making predictions.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/simple_ensemble.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Simple ensemble — simple_ensemble","text":"","code":"simple_ensemble(...)"},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/simple_ensemble.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Simple ensemble — simple_ensemble","text":"... used, function just creates empty simple_ensemble object. Members added add_best_candidates()","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/simple_ensemble.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Simple ensemble — simple_ensemble","text":"empty simple_ensemble. tibble columns: /itemize: /item: wflow_id: name workflows best model chosen /item: workflow: trained workflow objects /item: metrics: metrics based crossvalidation resampling used tune models","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/spatial_initial_split.html","id":null,"dir":"Reference","previous_headings":"","what":"Simple Training/Test Set Splitting for spatial data — spatial_initial_split","title":"Simple Training/Test Set Splitting for spatial data — spatial_initial_split","text":"spatial_initial_split creates single binary split data training set testing set. strategies package spatialsample available; random split strategy used generate initial split.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/spatial_initial_split.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Simple Training/Test Set Splitting for spatial data — spatial_initial_split","text":"","code":"spatial_initial_split(data, prop, strategy, ...)"},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/spatial_initial_split.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Simple Training/Test Set Splitting for spatial data — spatial_initial_split","text":"data dataset (data.frame tibble) prop proportion data retained modeling/analysis. strategy sampling strategy spatialsample ... parameters passed strategy","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/spatial_initial_split.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Simple Training/Test Set Splitting for spatial data — spatial_initial_split","text":"rsplit object can used rsample::training rsample::testing functions extract data split.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/spatial_initial_split.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Simple Training/Test Set Splitting for spatial data — spatial_initial_split","text":"","code":"set.seed(123) block_initial <- spatial_initial_split(boston_canopy, prop = 1/5, spatial_block_cv) testing(block_initial) #> Simple feature collection with 152 features and 18 fields #> Geometry type: MULTIPOLYGON #> Dimension:     XY #> Bounding box:  xmin: 745098 ymin: 2915630 xmax: 805045.8 ymax: 2969840 #> Projected CRS: NAD83 / Massachusetts Mainland (ftUS) #> # A tibble: 152 × 19 #>    grid_id land_area canopy_gain canopy_loss canopy_no_change canopy_area_2014 #>    <chr>       <dbl>       <dbl>       <dbl>            <dbl>            <dbl> #>  1 M-9      2690727.      52443.      53467.          304239.          357706. #>  2 Q-21     2690727.      54712.     101816.         1359305.         1461121. #>  3 AB-23     725043.      13737.      13278.           52628.           65906. #>  4 AC-15    1175032.      24517.      24010.          111148.          135158. #>  5 U-25     2691491.      83740.     117496.          601040.          718536. #>  6 Y-13     2691490.      79215.      41676.          312299.          353975. #>  7 M-10     2578879.      27026.      41240.          161115.          202355. #>  8 T-22     2691490.      80929.     140490.          573628.          714118. #>  9 AO-16    1717547.      64863.      52390.          465563.          517953. #> 10 X-23     2690728.      85198.     109044.          458205.          567249. #> # ℹ 142 more rows #> # ℹ 13 more variables: canopy_area_2019 <dbl>, change_canopy_area <dbl>, #> #   change_canopy_percentage <dbl>, canopy_percentage_2014 <dbl>, #> #   canopy_percentage_2019 <dbl>, change_canopy_absolute <dbl>, #> #   mean_temp_morning <dbl>, mean_temp_evening <dbl>, mean_temp <dbl>, #> #   mean_heat_index_morning <dbl>, mean_heat_index_evening <dbl>, #> #   mean_heat_index <dbl>, geometry <MULTIPOLYGON [US_survey_foot]> training(block_initial) #> Simple feature collection with 530 features and 18 fields #> Geometry type: MULTIPOLYGON #> Dimension:     XY #> Bounding box:  xmin: 739826.9 ymin: 2908294 xmax: 812069.7 ymax: 2970073 #> Projected CRS: NAD83 / Massachusetts Mainland (ftUS) #> # A tibble: 530 × 19 #>    grid_id land_area canopy_gain canopy_loss canopy_no_change canopy_area_2014 #>    <chr>       <dbl>       <dbl>       <dbl>            <dbl>            <dbl> #>  1 AB-4      795045.      15323.       3126.           53676.           56802. #>  2 I-33      265813.       8849.      11795.           78677.           90472. #>  3 AO-9      270153        6187.       1184.           26930.           28114. #>  4 H-10     2691490.      73098.      80362.          345823.          426185. #>  5 V-7       107890.        219.       3612.             240.            3852. #>  6 Q-22     2648089.     122211.     154236.         1026632.         1180868. #>  7 X-4       848558.       8275.       1760.            6872.            8632. #>  8 P-18     2690726.     110928.     113146.          915137.         1028283. #>  9 J-29     2574479.      38069.      15530.         2388638.         2404168. #> 10 G-28     2641525.      87024.      39246.         1202528.         1241774. #> # ℹ 520 more rows #> # ℹ 13 more variables: canopy_area_2019 <dbl>, change_canopy_area <dbl>, #> #   change_canopy_percentage <dbl>, canopy_percentage_2014 <dbl>, #> #   canopy_percentage_2019 <dbl>, change_canopy_absolute <dbl>, #> #   mean_temp_morning <dbl>, mean_temp_evening <dbl>, mean_temp <dbl>, #> #   mean_heat_index_morning <dbl>, mean_heat_index_evening <dbl>, #> #   mean_heat_index <dbl>, geometry <MULTIPOLYGON [US_survey_foot]>"},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/thin_by_cell.html","id":null,"dir":"Reference","previous_headings":"","what":"Thin point dataset to have 1 observation per raster cell — thin_by_cell","title":"Thin point dataset to have 1 observation per raster cell — thin_by_cell","text":"function thinss dataset one observation per cell retained.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/thin_by_cell.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Thin point dataset to have 1 observation per raster cell — thin_by_cell","text":"","code":"thin_by_cell(data, raster, coords = NULL, drop_na = TRUE, agg_fact = NULL)"},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/thin_by_cell.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Thin point dataset to have 1 observation per raster cell — thin_by_cell","text":"data sf::sf data frame, data frame coordinate variables. can defined coords, unless standard names (see details ). raster terra::SpatRaster object defined grid coords vector length two giving names \"x\" \"y\" coordinates, found data. left NULL, function try guess columns based standard names c(\"x\", \"y\"), c(\"X\",\"Y\"), c(\"longitude\", \"latitude\"), c(\"lon\", \"lat\") drop_na boolean whether locations NA raster dropped. agg_fact positive integer. Aggregation factor expressed number cells direction (horizontally vertically). two integers (horizontal vertical aggregation factor) three integers (also aggregating layers). Defaults NULL, implies aggregation (.e. thinning done grid raster)","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/thin_by_cell.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Thin point dataset to have 1 observation per raster cell — thin_by_cell","text":"object class sf::sf data.frame, \"data\".","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/thin_by_cell.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Thin point dataset to have 1 observation per raster cell — thin_by_cell","text":"thinning can achieved aggregating cells raster thinning, achieved setting agg_fact > 1 (aggregation works manner equivalent terra::aggregate()).","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/thin_by_cell_time.html","id":null,"dir":"Reference","previous_headings":"","what":"Thin point dataset to have 1 observation per raster cell per time slice — thin_by_cell_time","title":"Thin point dataset to have 1 observation per raster cell per time slice — thin_by_cell_time","text":"function thins dataset one observation per cell per time slice retained.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/thin_by_cell_time.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Thin point dataset to have 1 observation per raster cell per time slice — thin_by_cell_time","text":"","code":"thin_by_cell_time(   data,   raster,   coords = NULL,   time_col = \"time\",   lubridate_fun = c,   drop_na = TRUE,   agg_fact = NULL )"},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/thin_by_cell_time.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Thin point dataset to have 1 observation per raster cell per time slice — thin_by_cell_time","text":"data sf::sf data frame, data frame coordinate variables. can defined coords, unless standard names (see details ). raster terra::SpatRaster object defined grid coords vector length two giving names \"x\" \"y\" coordinates, found data. left NULL, function try guess columns based standard names c(\"x\", \"y\"), c(\"X\",\"Y\"), c(\"longitude\", \"latitude\"), c(\"lon\", \"lat\") time_col name column time; time lubridate object, use lubridate_fun provide function can used convert appropriately lubridate_fun function convert time column lubridate object drop_na boolean whether locations NA raster dropped. agg_fact positive integer. Aggregation factor expressed number cells direction (horizontally vertically). two integers (horizontal vertical aggregation factor) three integers (also aggregating layers). Defaults NULL, implies aggregation (.e. thinning done grid raster)","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/thin_by_cell_time.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Thin point dataset to have 1 observation per raster cell per time slice — thin_by_cell_time","text":"object class sf::sf data.frame, \"data\".","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/thin_by_cell_time.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Thin point dataset to have 1 observation per raster cell per time slice — thin_by_cell_time","text":"thinning can achieved aggregating cells raster thinning, achieved setting agg_fact > 1 (aggregation works manner equivalent terra::aggregate()).","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/thin_by_dist.html","id":null,"dir":"Reference","previous_headings":"","what":"Thin points dataset based on geographic distance — thin_by_dist","title":"Thin points dataset based on geographic distance — thin_by_dist","text":"function thins dataset observations distance greater \"dist_min\" retained.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/thin_by_dist.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Thin points dataset based on geographic distance — thin_by_dist","text":"","code":"thin_by_dist(data, dist_min, coords = NULL)"},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/thin_by_dist.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Thin points dataset based on geographic distance — thin_by_dist","text":"data sf::sf data frame, data frame coordinate variables. can defined coords, unless standard names (see details ). dist_min Minimum distance points (units appropriate projection, meters lonlat data). coords vector length two giving names \"x\" \"y\" coordinates, found data. left NULL, function try guess columns based standard names c(\"x\", \"y\"), c(\"X\",\"Y\"), c(\"longitude\", \"latitude\"), c(\"lon\", \"lat\")","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/thin_by_dist.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Thin points dataset based on geographic distance — thin_by_dist","text":"object class sf::sf data.frame, \"data\".","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/thin_by_dist.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Thin points dataset based on geographic distance — thin_by_dist","text":"Distances measured appropriate units projection used. case raw latitude longitude (e.g. provided data.frame), crs set WGS84, units set meters. function modified version algorithm spThin, adapted work sf objects.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/thin_by_dist_time.html","id":null,"dir":"Reference","previous_headings":"","what":"Thin points dataset based on geographic and temporal distance — thin_by_dist_time","title":"Thin points dataset based on geographic and temporal distance — thin_by_dist_time","text":"function thins dataset observations distance greater \"dist_min\" space \"interval_min\" time retained.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/thin_by_dist_time.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Thin points dataset based on geographic and temporal distance — thin_by_dist_time","text":"","code":"thin_by_dist_time(   data,   dist_min,   interval_min,   coords = NULL,   time_col = \"time\",   lubridate_fun = c )"},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/thin_by_dist_time.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Thin points dataset based on geographic and temporal distance — thin_by_dist_time","text":"data sf::sf data frame, data frame coordinate variables. can defined coords, unless standard names (see details ). dist_min Minimum distance points (units appropriate projection, meters lonlat data). interval_min Minimum time interval points, days. coords vector length two giving names \"x\" \"y\" coordinates, found data. left NULL, function try guess columns based standard names c(\"x\", \"y\"), c(\"X\",\"Y\"), c(\"longitude\", \"latitude\"), c(\"lon\", \"lat\") time_col name column time; time lubridate object, use lubridate_fun provide function can used convert appropriately lubridate_fun function convert time column lubridate object","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/thin_by_dist_time.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Thin points dataset based on geographic and temporal distance — thin_by_dist_time","text":"object class sf::sf data.frame, \"data\".","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/thin_by_dist_time.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Thin points dataset based on geographic and temporal distance — thin_by_dist_time","text":"Geographic distances measured appropriate units projection used. case raw latitude longitude (e.g. provided data.frame), crs set WGS84, units set meters. Time interval estimated days. Note long time period, simple converstion x years = 365 * x days might lead slightly shorter intervals expected, ignores leap years. function y2d() provides closer approximation. function algorithm analogous spThin, exception neighbours defined terms space time.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/tidysdm.html","id":null,"dir":"Reference","previous_headings":"","what":"tidysdm — tidysdm","title":"tidysdm — tidysdm","text":"R library facilitates fitting Species Distribution Models tidymodels","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/tss.html","id":null,"dir":"Reference","previous_headings":"","what":"TSS - True Skill Statistics — tss","title":"TSS - True Skill Statistics — tss","text":"True Skills Statistic, defined ","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/tss.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"TSS - True Skill Statistics — tss","text":"","code":"tss(data, ...)  # S3 method for data.frame tss(   data,   truth,   estimate,   estimator = NULL,   na_rm = TRUE,   case_weights = NULL,   event_level = \"first\",   ... )"},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/tss.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"TSS - True Skill Statistics — tss","text":"data Either data.frame containing columns specified truth estimate arguments, table/matrix true class results columns table. ... currently used. truth column identifier true class results (factor). unquoted column name although argument passed expression supports quasiquotation (can unquote column names). ⁠_vec()⁠ functions, factor vector. estimate column identifier predicted class results (also factor). truth can specified different ways primary method use unquoted variable name. ⁠_vec()⁠ functions, factor vector. estimator One : \"binary\", \"macro\", \"macro_weighted\", \"micro\" specify type averaging done. \"binary\" relevant two class case. three general methods calculating multiclass metrics. default automatically choose \"binary\" \"macro\" based estimate. na_rm logical value indicating whether NA values stripped computation proceeds. case_weights optional column identifier case weights. unquoted column name evaluates numeric column data. ⁠_vec()⁠ functions, numeric vector. event_level single string. Either \"first\" \"second\" specify level truth consider \"event\". argument applicable estimator = \"binary\". default \"first\".","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/tss.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"TSS - True Skill Statistics — tss","text":"tibble columns .metric, .estimator, .estimate 1 row values. grouped data frames, number rows returned number groups.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/tss.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"TSS - True Skill Statistics — tss","text":"sensitivity+specificity +1 function wrapper around yardstick::j_index(), another name quantity. Note function takes classes predicted model without calibration (.e. making split 0.5 probability). usually metric used Species Distribution Models, threshold recalibrated maximise TSS; purpose, use tss_max().","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/tss.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"TSS - True Skill Statistics — tss","text":"","code":"#Two class data(\"two_class_example\") tss(two_class_example, truth, predicted) #> # A tibble: 1 × 3 #>   .metric .estimator .estimate #>   <chr>   <chr>          <dbl> #> 1 tss     binary         0.673 # Multiclass library(dplyr) data(hpc_cv) # Groups are respected hpc_cv %>%  group_by(Resample) %>%  tss(obs, pred) #> # A tibble: 10 × 4 #>    Resample .metric .estimator .estimate #>    <chr>    <chr>   <chr>          <dbl> #>  1 Fold01   tss     macro          0.434 #>  2 Fold02   tss     macro          0.422 #>  3 Fold03   tss     macro          0.533 #>  4 Fold04   tss     macro          0.449 #>  5 Fold05   tss     macro          0.431 #>  6 Fold06   tss     macro          0.413 #>  7 Fold07   tss     macro          0.398 #>  8 Fold08   tss     macro          0.468 #>  9 Fold09   tss     macro          0.435 #> 10 Fold10   tss     macro          0.412"},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/tss_max.html","id":null,"dir":"Reference","previous_headings":"","what":"Maximum TSS - True Skill Statistics — tss_max","title":"Maximum TSS - True Skill Statistics — tss_max","text":"True Skills Statistic, defined ","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/tss_max.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Maximum TSS - True Skill Statistics — tss_max","text":"","code":"tss_max(data, ...)  # S3 method for data.frame tss_max(   data,   truth,   ...,   estimator = NULL,   na_rm = TRUE,   event_level = \"first\",   case_weights = NULL )  # S3 method for sf tss_max(data, ...)  tss_max_vec(   truth,   estimate,   estimator = NULL,   na_rm = TRUE,   event_level = \"first\",   case_weights = NULL,   ... )"},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/tss_max.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Maximum TSS - True Skill Statistics — tss_max","text":"data Either data.frame containing columns specified truth estimate arguments, table/matrix true class results columns table. ... set unquoted column names one dplyr selector functions choose variables contain class probabilities. truth binary, 1 column selected, correspond value event_level. Otherwise, many columns factor levels truth ordering columns factor levels truth. truth column identifier true class results (factor). unquoted column name although argument passed expression supports quasiquotation (can unquote column names). ⁠_vec()⁠ functions, factor vector. estimator One \"binary\", \"hand_till\", \"macro\", \"macro_weighted\" specify type averaging done. \"binary\" relevant two class case. others general methods calculating multiclass metrics. default automatically choose \"binary\" truth binary, \"hand_till\" truth >2 levels case_weights specified, \"macro\" truth >2 levels case_weights specified (case \"hand_till\" well-defined). na_rm logical value indicating whether NA values stripped computation proceeds. event_level single string. Either \"first\" \"second\" specify level truth consider \"event\". argument applicable estimator = \"binary\". default uses internal helper generally defaults \"first\" case_weights optional column identifier case weights. unquoted column name evaluates numeric column data. ⁠_vec()⁠ functions, numeric vector. estimate truth binary, numeric vector class probabilities corresponding \"relevant\" class. Otherwise, matrix many columns factor levels truth. assumed order levels truth.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/tss_max.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Maximum TSS - True Skill Statistics — tss_max","text":"tibble columns .metric, .estimator, .estimate 1 row values. grouped data frames, number rows returned number groups.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/tss_max.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Maximum TSS - True Skill Statistics — tss_max","text":"sensitivity+specificity +1 function calibrates probability threshold classify presences maximise TSS. multiclass version function, operates binary predictions (e.g. presences absences SDMs).","code":""},{"path":[]},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/tss_max.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Maximum TSS - True Skill Statistics — tss_max","text":"","code":"tss_max(two_class_example, truth, Class1) #> # A tibble: 1 × 3 #>   .metric .estimator .estimate #>   <chr>   <chr>          <dbl> #> 1 tss_max binary         0.728"},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/y2d.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert a time interval from years to days — y2d","title":"Convert a time interval from years to days — y2d","text":"function takes takes time interval years converts days, unit commonly used time operations R. simple conversion x * 365 work large number years, due presence leap years.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/y2d.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert a time interval from years to days — y2d","text":"","code":"y2d(x)"},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/y2d.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert a time interval from years to days — y2d","text":"x number years interval","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/y2d.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Convert a time interval from years to days — y2d","text":"difftime object (days)","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/y2d.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Convert a time interval from years to days — y2d","text":"","code":"y2d(1) #> Time difference of 365 days y2d(1000) #> Time difference of 365243 days"},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/ybp2date.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert years BP from pastclim to lubridate date, or viceversa — ybp2date","title":"Convert years BP from pastclim to lubridate date, or viceversa — ybp2date","text":"functions convert years BP used pastclim (negative numbers going past, positive future) standard POSIXct date objects.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/ybp2date.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert years BP from pastclim to lubridate date, or viceversa — ybp2date","text":"","code":"ybp2date(x)  date2ybp(x)"},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/ybp2date.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert years BP from pastclim to lubridate date, or viceversa — ybp2date","text":"x time years BP using pastclim convention negative numbers indicating years past, POSIXct date object","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/ybp2date.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Convert years BP from pastclim to lubridate date, or viceversa — ybp2date","text":"POSIXct date object, vector","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/dev/reference/ybp2date.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Convert years BP from pastclim to lubridate date, or viceversa — ybp2date","text":"","code":"ybp2date(-10000) #> [1] \"-8050-01-01 UTC\" ybp2date(0) #> [1] \"1950-01-01 UTC\" # back and forth date2ybp(ybp2date(-10000)) #> [1] -10000"}]
